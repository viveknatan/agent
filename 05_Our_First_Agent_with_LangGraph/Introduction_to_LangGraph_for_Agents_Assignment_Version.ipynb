{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- 🤝 Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "\n",
        "- 🤝 Breakout Room #2:\n",
        "  1. Evaluating the LangGraph Application with LangSmith\n",
        "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
        "  3. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# 🤝 Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries.\n",
        "\n",
        "> NOTE: If you're running this locally - please skip this step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "ba50f775-3957-4d88-9a88-43acc6966dda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.8/145.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.4/412.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#!pip install -qU langchain langchain_openai langchain-community langgraph arxiv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "3fa78560-393c-4ee5-b871-9886bf0d70f4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkla2fpx28QK",
        "outputId": "52d7ad22-fcb1-4abe-853b-216c55a12650"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "b69df90a-b4e1-4ddb-9de0-882d98b68ab2"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE6 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Tavily Search Results](https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/tools/tavily_search/tool.py)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "#### 🏗️ Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "from langchain_community.tools.wikipedia.tool import WikipediaQueryRun\n",
        "from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n",
        "\n",
        "# Initialize the Wikipedia API wrapper\n",
        "wikipedia_api_wrapper = WikipediaAPIWrapper()\n",
        "\n",
        "# Initialize the tools\n",
        "tavily_tool = TavilySearchResults(max_results=5)\n",
        "arxiv_tool = ArxivQueryRun()\n",
        "wikipedia_tool = WikipediaQueryRun(api_wrapper=wikipedia_api_wrapper)\n",
        "\n",
        "# Assemble the tool belt\n",
        "tool_belt = [\n",
        "    tavily_tool\n",
        "    , arxiv_tool\n",
        "    , wikipedia_tool\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Page: History of the Internet\\nSummary: The history of the Internet originated in the efforts of scientists and engineers to build and interconnect computer networks. The Internet Protocol Suite, the set of rules used to communicate between networks and devices on the Internet, arose from research and development in the United States and involved international collaboration, particularly with researchers in the United Kingdom and France.\\nComputer science was an emerging discipline in the late 1950s that began to consider time-sharing between computer users, and later, the possibility of achieving this over wide area networks. J. C. R. Licklider developed the idea of a universal network at the Information Processing Techniques Office (IPTO) of the United States Department of Defense (DoD) Advanced Research Projects Agency (ARPA). Independently, Paul Baran at the RAND Corporation proposed a distributed network based on data in message blocks in the early 1960s, and Donald Davies conceived of packet switching in 1965 at the National Physical Laboratory (NPL), proposing a national commercial data network in the United Kingdom.\\nARPA awarded contracts in 1969 for the development of the ARPANET project, directed by Robert Taylor and managed by Lawrence Roberts. ARPANET adopted the packet switching technology proposed by Davies and Baran. The network of Interface Message Processors (IMPs) was built by a team at Bolt, Beranek, and Newman, with the design and specification led by Bob Kahn. The host-to-host protocol was specified by a group of graduate students at UCLA, led by Steve Crocker, along with Jon Postel and others. The ARPANET expanded rapidly across the United States with connections to the United Kingdom and Norway.\\nSeveral early packet-switched networks emerged in the 1970s which researched and provided data networking. Louis Pouzin and Hubert Zimmermann pioneered a simplified end-to-end approach to internetworking at the IRIA. Peter Kirstein put internetworking into practice at University College London in 1973. Bob Metcalfe developed the theory behind Ethernet and the PARC Universal Packet. ARPA initiatives and the International Network Working Group developed and refined ideas for internetworking, in which multiple separate networks could be joined into a network of networks. Vint Cerf, now at Stanford University, and Bob Kahn, now at DARPA, published their research on internetworking in 1974. Through the Internet Experiment Note series and later RFCs this evolved into the Transmission Control Protocol (TCP) and Internet Protocol (IP), two protocols of the Internet protocol suite. The design included concepts pioneered in the French CYCLADES project directed by Louis Pouzin. The development of packet switching networks was underpinned by mathematical work in the 1970s by Leonard Kleinrock at UCLA.\\n\\nIn the late 1970s, national and international public data networks emerged based on the X.25 protocol, designed by Rémi Després and others. In the United States, the National Science Foundation (NSF) funded national supercomputing centers at several universities in the United States, and provided interconnectivity in 1986 with the NSFNET project, thus creating network access to these supercomputer sites for research and academic organizations in the United States. International connections to NSFNET, the emergence of architecture such as the Domain Name System, and the adoption of TCP/IP on existing networks in the United States and around the world marked the beginnings of the Internet. Commercial Internet service providers (ISPs) emerged in 1989 in the United States and Australia. Limited private connections to parts of the Internet by officially commercial entities emerged in several American cities by late 1989 and 1990. The optical backbone of the NSFNET was decommissioned in 1995, removing the last restrictions on the use of the Internet to carry commercial traffic, as traffic transitioned to optical networks managed by Sp'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wikipedia_tool.invoke({'query': 'History of the Internet'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "model = model.bind_tools(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ❓ Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "The model in this case is provided with a list of tools that it can use. The LLM decides which tool to call based on the user's input and the description of the tool. The model scores each tool based on the user's intent and then selects the one which scores the highest. Also as the model has context awareness, it puts that knowledge to use as well. In conclusion, it is learned behavior from fine-tuning on many examples of when to use which tool based on intent and tool signature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]\n",
        "  steps_taken: int  # New field to track cycles\n",
        "  history: list[str]  # ✅ NEW: track steps taken\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  steps = state.get(\"steps_taken\", 0)\n",
        "  history = state.get(\"history\", [])\n",
        "\n",
        "  response = model.invoke(messages)\n",
        "\n",
        "  return {\n",
        "    \"messages\" : [response],\n",
        "    \"steps_taken\": steps + 1, \n",
        "    \"history\": history + [f\"Step {steps + 1}: Agent called model.\"]\n",
        "  }\n",
        "\n",
        "tool_node = ToolNode(tool_belt)\n",
        "\n",
        "def call_tool(state: AgentState) -> AgentState:\n",
        "    new_state = tool_node.invoke(state)\n",
        "    history = state.get(\"history\", [])\n",
        "    steps = state.get(\"steps_taken\", 0)\n",
        "\n",
        "    new_state[\"history\"] = history + [f\"Step {steps}: Tool executed.\"]\n",
        "    return new_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[TavilySearchResults(api_wrapper=TavilySearchAPIWrapper(tavily_api_key=SecretStr('**********'))),\n",
              " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=3, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=4000)),\n",
              " WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/Users/viveknatan/Documents/AIMCourse/agent/05_Our_First_Agent_with_LangGraph/.venv/lib/python3.13/site-packages/wikipedia/__init__.py'>, top_k_results=3, lang='en', load_all_available_meta=False, doc_content_chars_max=4000))]"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tool_belt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `tool_node` is a node which can call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vF4_lgtmQNo",
        "outputId": "a4384377-8f7a-415f-be1b-fee6169cb101"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x129725550>"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "uncompiled_graph = StateGraph(AgentState)\n",
        "\n",
        "uncompiled_graph.add_node(\"agent\", call_model)\n",
        "uncompiled_graph.add_node(\"action\", call_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGCbaYqRnmiw",
        "outputId": "5351807c-2ac7-4316-a3a3-878abeacd114"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x129725550>"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uncompiled_graph.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BZgb81VQf9o",
        "outputId": "73a07c15-5f0b-40f2-b033-38b57d056dd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x129725550>"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def should_continue(state: AgentState) -> str:\n",
        "    steps = state.get(\"steps_taken\", 0)\n",
        "    if steps >= 5:\n",
        "        state[\"history\"].append(f\"Step {steps}: Reached max step limit. Ending.\")\n",
        "        return END\n",
        "\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    if last_message.tool_calls:\n",
        "        return \"action\"\n",
        "\n",
        "    state[\"history\"].append(f\"Step {steps}: No tool call. Ending.\")\n",
        "    return END\n",
        "\n",
        "uncompiled_graph.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvcgbHf1rIXZ",
        "outputId": "45d4bdd6-d6bb-4a1d-bb79-cad43c130bf2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x129725550>"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uncompiled_graph.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "compiled_graph = uncompiled_graph.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x12635cb90>"
            ]
          },
          "execution_count": 150,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compiled_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ❓ Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "5eeedfae-089d-496e-e71f-071939fa5832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_eMO3u4HIUeV6lP6D0PTfaT2A', 'function': {'arguments': '{\"query\":\"current captain of T20 Indian Cricket Team 2023\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 223, 'total_tokens': 252, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a6889ffe71', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-b013f0fa-9fdb-4835-bd36-7b0f01926c0a-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current captain of T20 Indian Cricket Team 2023'}, 'id': 'call_eMO3u4HIUeV6lP6D0PTfaT2A', 'type': 'tool_call'}], usage_metadata={'input_tokens': 223, 'output_tokens': 29, 'total_tokens': 252, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='[{\"url\": \"https://en.wikipedia.org/wiki/List_of_India_national_cricket_captains\", \"content\": \"Captain ofIndia national cricket team\\\\nOfficial Flag of India\\\\nIncumbentTop:Rohit Sharma(Men\\'s team: Test and ODI) andSuryakumar Yadav(Men\\'s Team: T20I)Bottom:Harmanpreet Kaur(Women\\'s team - All formats)since\\xa0men\\'s Team ODI: 2017 and Test: February 2022; T20I: 23 November 2023women\\'s Team All Formats; 2012/13 Respectively\\\\nBoard of Control for Cricket in India\\\\nType | Captain\\\\nStatus | Captain of theIndia National Cricket Team [...] 21 |  | Gautam Gambhir | 2010–11 | 6 | 6 | 0 | 0 | 0 | 100.00\\\\n22 |  | Virat Kohli | 2013–21 | 95 | 65 | 27 | 1 | 2 | 68.42\\\\n23 |  | Ajinkya Rahane | 2015 | 3 | 3 | 0 | 0 | 0 | 100.00\\\\n24 |  | Rohit Sharma | 2017–present | 56 | 42 | 12 | 1 | 1 | 75.00\\\\n25 |  | Shikhar Dhawan | 2021–22 | 12 | 7 | 3 | 0 | 2 | 58.33\\\\n26 |  | KL Rahul | 2022–23 | 12 | 8 | 4 | 0 | 0 | 66.67\\\\n27 |  | Hardik Pandya | 2023 | 3 | 2 | 1 | 0 | 0 | 66.67\\\\nTotal | 1066 | 567 | 445 | 10 | 44 | 53.18\\\\nT20I [...] 7 |  | Shikhar Dhawan | 2021 | 3 | 1 | 2 | 0 | 0 | 33.33\\\\n8 |  | Rishabh Pant | 2022 | 5 | 2 | 2 | 0 | 1 | 50.00\\\\n9 |  | Hardik Pandya | 2022-2023 | 16 | 10 | 5 | 1 | 0 | 65.52\\\\n10 |  | KL Rahul | 2022 | 1 | 1 | 0 | 0 | 0 | 100.00\\\\n11 |  | Jasprit Bumrah | 2023 | 2 | 2 | 0 | 0 | 0 | 100.00\\\\n12 |  | Ruturaj Gaikwad | 2023 | 3 | 2 | 0 | 0 | 1 | 66.67\\\\n13 |  | Suryakumar Yadav | 2023–present | 19 | 16 | 3 | 0 | 0 | 84.21\\\\n14 |  | Shubman Gill | 2024 | 5 | 4 | 1 | 0 | 0 | 80.00\"}, {\"url\": \"https://www.espncricinfo.com/records/team/individual-list-captains/india-6/twenty20-internationals-3\", \"content\": \"List of Captains for India in T20Is ; MS Dhoni. 2007-2016, 72 ; V Kohli. 2017-2021, 50 ; HH Pandya. 2022-2023, 16 ; Shubman Gill. 2024-2024, 5\"}, {\"url\": \"https://www.icc-cricket.com/news/india-announce-new-captain-for-t20is-as-they-name-squad-for-sri-lanka-series\", \"content\": \"The T20I squad has a familiar look with several T20 World Cup stars like Hardik Pandya, Arshdeep Singh, Mohammed Siraj and Axar Patel returning after missing the Zimbabwe series.\\\\n\\\\nJasprit Bumrah has been rested from the tour while the likes of Rohit Sharma and Virat Kohli will play the ODIs.\\\\n\\\\nShubman Gill hits form with 92 | CWC23\\\\n\\\\nHighlights of Shubman Gill\\'s 92. The batter showed good form in Mumbai with a knock that included 11 fours and two sixes. [...] A new chapter commences for India\\'s T20I team under the guidance of their new head coach, marking the transition from the era of veterans Rohit Sharma, Virat Kohli and Ravindra Jadeja – all of whom retired from the format following their triumphant T20 World Cup victory.\\\\n\\\\nThe ODI squad features Riyan Parag, who made his debut in the T20I series in Zimbabwe. KL Rahul and Rishabh Pant are the wicketkeepers with Sanju Samson missing out. [...] Suryakumar Yadav, the No.2 ranked India batter in the ICC Men’s T20I Batting Rankings, has been named as the T20I captain ahead of the Sri Lanka series.\\\\n\\\\nThe middle-order batter, who has played 68 T20Is, led the team against Australia and South Africa last year in the absence of key players, but has now been handed the reins of the team in the shortest format.\"}, {\"url\": \"https://en.wikipedia.org/wiki/India_national_cricket_team\", \"content\": \"Africa.[167] Kohli later quit as Test captain as well, after their Test series loss to South Africa.[168] Sharma replaced Kohli as Test captain before the Test series against Sri Lanka[169] and is now the full-time captain of the Indian men\\'s cricket team. Rohit took India to the finals of the 2023 World Test Championship and 2023 World Cup, and victories in the 2024 T20 World Cup and 2025 Champions Trophy.[170] [...] Devdutt Padikkal | 24 | Right-handed | — | Karnataka | Royal Challengers Bengaluru | — | Test | 37 |  | 2024 | — | 2021\\\\nRohit Sharma | 37 | Right-handed | Right-armoff break | Mumbai | Mumbai Indians | A+ | Test, ODI | 45 | Test, ODI (C) | 2024 | 2025 | 2024\\\\nRinku Singh | 27 | Left-handed | Right-armoff break | Uttar Pradesh | Kolkata Knight Riders | C | T20I | 35 |  | — | 2023 | 2025 [...] Suryakumar Yadav | 34 | Right-handed | Right-armoff break | Mumbai | Mumbai Indians | B | T20I | 63 | T20I (C) | 2023 | 2023 | 2025\\\\nTilak Varma | 22 | Left-handed | Right-armoff break | Hyderabad | Mumbai Indians | C | T20I | 72 |  | — | 2023 | 2025\\\\nAll-rounders\\\\nShivam Dube | 31 | Left-handed | Right-armmedium | Mumbai | Chennai Super Kings | C | ODI, T20I | 25 |  | — | 2024 | 2025\"}, {\"url\": \"https://www.olympics.com/en/news/indian-cricket-team-squad-vs-england-t20-series-2025\", \"content\": \"Suryakumar Yadav will remain the captain of the Indian T20 team for the England T20 series with Axar Patel named as his deputy. The 15-member squad, announced by the Board of Control for Cricket in India (BCCI) on Saturday, includes Border-Gavaskar Trophy hero Nitish Kumar Reddy.\\\\n\\\\nSanju Samson, the highest run-scorer for the Indian cricket team last year in T20I cricket, is also a part of the squad.\"}]', name='tavily_search_results_json', id='5e19cc89-a334-42be-88d6-f4d0cf1a0e90', tool_call_id='call_eMO3u4HIUeV6lP6D0PTfaT2A', artifact={'query': 'current captain of T20 Indian Cricket Team 2023', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://en.wikipedia.org/wiki/List_of_India_national_cricket_captains', 'title': 'List of India national cricket captains - Wikipedia', 'content': \"Captain ofIndia national cricket team\\nOfficial Flag of India\\nIncumbentTop:Rohit Sharma(Men's team: Test and ODI) andSuryakumar Yadav(Men's Team: T20I)Bottom:Harmanpreet Kaur(Women's team - All formats)since\\xa0men's Team ODI: 2017 and Test: February 2022; T20I: 23 November 2023women's Team All Formats; 2012/13 Respectively\\nBoard of Control for Cricket in India\\nType | Captain\\nStatus | Captain of theIndia National Cricket Team [...] 21 |  | Gautam Gambhir | 2010–11 | 6 | 6 | 0 | 0 | 0 | 100.00\\n22 |  | Virat Kohli | 2013–21 | 95 | 65 | 27 | 1 | 2 | 68.42\\n23 |  | Ajinkya Rahane | 2015 | 3 | 3 | 0 | 0 | 0 | 100.00\\n24 |  | Rohit Sharma | 2017–present | 56 | 42 | 12 | 1 | 1 | 75.00\\n25 |  | Shikhar Dhawan | 2021–22 | 12 | 7 | 3 | 0 | 2 | 58.33\\n26 |  | KL Rahul | 2022–23 | 12 | 8 | 4 | 0 | 0 | 66.67\\n27 |  | Hardik Pandya | 2023 | 3 | 2 | 1 | 0 | 0 | 66.67\\nTotal | 1066 | 567 | 445 | 10 | 44 | 53.18\\nT20I [...] 7 |  | Shikhar Dhawan | 2021 | 3 | 1 | 2 | 0 | 0 | 33.33\\n8 |  | Rishabh Pant | 2022 | 5 | 2 | 2 | 0 | 1 | 50.00\\n9 |  | Hardik Pandya | 2022-2023 | 16 | 10 | 5 | 1 | 0 | 65.52\\n10 |  | KL Rahul | 2022 | 1 | 1 | 0 | 0 | 0 | 100.00\\n11 |  | Jasprit Bumrah | 2023 | 2 | 2 | 0 | 0 | 0 | 100.00\\n12 |  | Ruturaj Gaikwad | 2023 | 3 | 2 | 0 | 0 | 1 | 66.67\\n13 |  | Suryakumar Yadav | 2023–present | 19 | 16 | 3 | 0 | 0 | 84.21\\n14 |  | Shubman Gill | 2024 | 5 | 4 | 1 | 0 | 0 | 80.00\", 'score': 0.853046, 'raw_content': None}, {'url': 'https://www.espncricinfo.com/records/team/individual-list-captains/india-6/twenty20-internationals-3', 'title': 'India T20I matches individual list captains | ESPNcricinfo', 'content': 'List of Captains for India in T20Is ; MS Dhoni. 2007-2016, 72 ; V Kohli. 2017-2021, 50 ; HH Pandya. 2022-2023, 16 ; Shubman Gill. 2024-2024, 5', 'score': 0.776708, 'raw_content': None}, {'url': 'https://www.icc-cricket.com/news/india-announce-new-captain-for-t20is-as-they-name-squad-for-sri-lanka-series', 'title': 'India announce new captain for T20Is as they name squad for Sri ...', 'content': \"The T20I squad has a familiar look with several T20 World Cup stars like Hardik Pandya, Arshdeep Singh, Mohammed Siraj and Axar Patel returning after missing the Zimbabwe series.\\n\\nJasprit Bumrah has been rested from the tour while the likes of Rohit Sharma and Virat Kohli will play the ODIs.\\n\\nShubman Gill hits form with 92 | CWC23\\n\\nHighlights of Shubman Gill's 92. The batter showed good form in Mumbai with a knock that included 11 fours and two sixes. [...] A new chapter commences for India's T20I team under the guidance of their new head coach, marking the transition from the era of veterans Rohit Sharma, Virat Kohli and Ravindra Jadeja – all of whom retired from the format following their triumphant T20 World Cup victory.\\n\\nThe ODI squad features Riyan Parag, who made his debut in the T20I series in Zimbabwe. KL Rahul and Rishabh Pant are the wicketkeepers with Sanju Samson missing out. [...] Suryakumar Yadav, the No.2 ranked India batter in the ICC Men’s T20I Batting Rankings, has been named as the T20I captain ahead of the Sri Lanka series.\\n\\nThe middle-order batter, who has played 68 T20Is, led the team against Australia and South Africa last year in the absence of key players, but has now been handed the reins of the team in the shortest format.\", 'score': 0.7722049, 'raw_content': None}, {'url': 'https://en.wikipedia.org/wiki/India_national_cricket_team', 'title': 'India national cricket team - Wikipedia', 'content': \"Africa.[167] Kohli later quit as Test captain as well, after their Test series loss to South Africa.[168] Sharma replaced Kohli as Test captain before the Test series against Sri Lanka[169] and is now the full-time captain of the Indian men's cricket team. Rohit took India to the finals of the 2023 World Test Championship and 2023 World Cup, and victories in the 2024 T20 World Cup and 2025 Champions Trophy.[170] [...] Devdutt Padikkal | 24 | Right-handed | — | Karnataka | Royal Challengers Bengaluru | — | Test | 37 |  | 2024 | — | 2021\\nRohit Sharma | 37 | Right-handed | Right-armoff break | Mumbai | Mumbai Indians | A+ | Test, ODI | 45 | Test, ODI (C) | 2024 | 2025 | 2024\\nRinku Singh | 27 | Left-handed | Right-armoff break | Uttar Pradesh | Kolkata Knight Riders | C | T20I | 35 |  | — | 2023 | 2025 [...] Suryakumar Yadav | 34 | Right-handed | Right-armoff break | Mumbai | Mumbai Indians | B | T20I | 63 | T20I (C) | 2023 | 2023 | 2025\\nTilak Varma | 22 | Left-handed | Right-armoff break | Hyderabad | Mumbai Indians | C | T20I | 72 |  | — | 2023 | 2025\\nAll-rounders\\nShivam Dube | 31 | Left-handed | Right-armmedium | Mumbai | Chennai Super Kings | C | ODI, T20I | 25 |  | — | 2024 | 2025\", 'score': 0.76490957, 'raw_content': None}, {'url': 'https://www.olympics.com/en/news/indian-cricket-team-squad-vs-england-t20-series-2025', 'title': 'Indian cricket team for England T20 2025 series - full squad', 'content': 'Suryakumar Yadav will remain the captain of the Indian T20 team for the England T20 series with Axar Patel named as his deputy. The 15-member squad, announced by the Board of Control for Cricket in India (BCCI) on Saturday, includes Border-Gavaskar Trophy hero Nitish Kumar Reddy.\\n\\nSanju Samson, the highest run-scorer for the Indian cricket team last year in T20I cricket, is also a part of the squad.', 'score': 0.7040996, 'raw_content': None}], 'response_time': 3.26})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='The current captain of the T20 Indian Cricket Team is Suryakumar Yadav.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 1954, 'total_tokens': 1974, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a6889ffe71', 'finish_reason': 'stop', 'logprobs': None}, id='run-83a767d2-e3bf-432a-b312-e9b6fa7a2355-0', usage_metadata={'input_tokens': 1954, 'output_tokens': 20, 'total_tokens': 1974, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"Who is the current captain of the T20 Indian Cricket Team?\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "ff009536-d281-4a56-c126-9cd245352bfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_VddaXPVk1kaqiRQGOxRupC3C', 'function': {'arguments': '{\"query\":\"QLoRA\"}', 'name': 'arxiv'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 248, 'total_tokens': 265, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a6889ffe71', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-da079734-62e1-42f5-b163-f8b0a7a0e58e-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'QLoRA'}, 'id': 'call_VddaXPVk1kaqiRQGOxRupC3C', 'type': 'tool_call'}], usage_metadata={'input_tokens': 248, 'output_tokens': 17, 'total_tokens': 265, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: arxiv\n",
            "[ToolMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2025-02-05\\nTitle: Resource-Efficient & Effective Code Summarization\\nAuthors: Saima Afrin, Joseph Call, Khai-Nguyen Nguyen, Oscar Chaparro, Antonio Mastropaolo\\nSummary: Code Language Models (CLMs) have demonstrated high effectiveness in\\nautomating software engineering tasks such as bug fixing, code generation, and\\ncode documentation. This ', name='arxiv', id='f1d681a3-6997-4fa8-a906-5e3dddaedac7', tool_call_id='call_VddaXPVk1kaqiRQGOxRupC3C')]\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_pPr2t1AMJh4MK9DbrGQgZjr3', 'function': {'arguments': '{\"query\": \"Tim Dettmers\"}', 'name': 'wikipedia'}, 'type': 'function'}, {'id': 'call_O1Hn55GXx4HrT1C1jCGYbzhW', 'function': {'arguments': '{\"query\": \"Artidoro Pagnoni\"}', 'name': 'wikipedia'}, 'type': 'function'}, {'id': 'call_USiSpwobQ4wL6U2dvtZtvbZL', 'function': {'arguments': '{\"query\": \"Ari Holtzman\"}', 'name': 'wikipedia'}, 'type': 'function'}, {'id': 'call_J6khaFAxpGsraWHvLwud5p0y', 'function': {'arguments': '{\"query\": \"Luke Zettlemoyer\"}', 'name': 'wikipedia'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 85, 'prompt_tokens': 1218, 'total_tokens': 1303, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1152}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a6889ffe71', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-1cb25b5c-6473-4fff-8bb4-969aed96ccc5-0', tool_calls=[{'name': 'wikipedia', 'args': {'query': 'Tim Dettmers'}, 'id': 'call_pPr2t1AMJh4MK9DbrGQgZjr3', 'type': 'tool_call'}, {'name': 'wikipedia', 'args': {'query': 'Artidoro Pagnoni'}, 'id': 'call_O1Hn55GXx4HrT1C1jCGYbzhW', 'type': 'tool_call'}, {'name': 'wikipedia', 'args': {'query': 'Ari Holtzman'}, 'id': 'call_USiSpwobQ4wL6U2dvtZtvbZL', 'type': 'tool_call'}, {'name': 'wikipedia', 'args': {'query': 'Luke Zettlemoyer'}, 'id': 'call_J6khaFAxpGsraWHvLwud5p0y', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1218, 'output_tokens': 85, 'total_tokens': 1303, 'input_token_details': {'audio': 0, 'cache_read': 1152}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: wikipedia\n",
            "[ToolMessage(content='Page: Data parallelism\\nSummary: Data parallelism is parallelization across multiple processors in parallel computing environments. It focuses on distributing the data across different nodes, which operate on the data in parallel. It can be applied on regular data structures like arrays and matrices by working on each element in parallel. It contrasts to task parallelism as another form of parallelism.\\nA data parallel job on an array of n elements can be divided equally among all the processors. Let us assume we want to sum all the elements of the given array and the time for a single addition operation is Ta time units. In the case of sequential execution, the time taken by the process will be n×Ta time units as it sums up all the elements of an array. On the other hand, if we execute this job as a data parallel job on 4 processors the time taken would reduce to (n/4)×Ta + merging overhead time units. Parallel execution results in a speedup of 4 over sequential execution. The locality of data references plays an important part in evaluating the performance of a data parallel programming model. Locality of data depends on the memory accesses performed by the program as well as the size of the cache.\\n\\nPage: Ari Holtzman\\nSummary: Ari Holtzman is a professor of Computer Science at the University of Chicago and an expert in the area of Natural language processing and Computational linguistics. Previously, Holtzman was a PhD student at the University of Washington where he was advised by Luke Zettlemoyer.\\nIn 2017, he was a member of the winning team for the inaugural Alexa Prize for developing a conversational AI system for the Amazon Alexa device. Holtzman has made multiple contributions in the area of text generation and language models such as the introduction of nucleus sampling in 2019, his work on AI safety and neural fake news detection, and the fine-tuning of quantized large language models.\\n\\nPage: Bob Dettmer\\nSummary: Robert P. \"Bob\" Dettmer (born March 1, 1951) is a Minnesota politician and former member of the Minnesota House of Representatives. A member of the Republican Party of Minnesota, he represented District 39A, located in the northeastern part of the Twin Cities metro area, which includes Chisago and Washington counties.\\n\\n', name='wikipedia', id='878ea698-be81-49b6-a50c-31dd6496b03a', tool_call_id='call_pPr2t1AMJh4MK9DbrGQgZjr3'), ToolMessage(content='Page: Ari Holtzman\\nSummary: Ari Holtzman is a professor of Computer Science at the University of Chicago and an expert in the area of Natural language processing and Computational linguistics. Previously, Holtzman was a PhD student at the University of Washington where he was advised by Luke Zettlemoyer.\\nIn 2017, he was a member of the winning team for the inaugural Alexa Prize for developing a conversational AI system for the Amazon Alexa device. Holtzman has made multiple contributions in the area of text generation and language models such as the introduction of nucleus sampling in 2019, his work on AI safety and neural fake news detection, and the fine-tuning of quantized large language models.\\n\\nPage: Large language model\\nSummary: A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\\n\\nPage: ML.NET\\nSummary: ML.NET is a free software machine learning library for the C# and F# programming languages. It also supports Python models when used together with NimbusML. The preview release of ML.NET included transforms for feature engineering like n-gram creation, and learners to handle binary classification, multi-class classification, and regression tasks. Additional ML tasks like anomaly detection and recommendation systems have since been added, and other approaches like deep learning will be included in future versions.\\n\\n', name='wikipedia', id='d8345f72-41d8-4bcc-8c34-367c1ac27a1e', tool_call_id='call_O1Hn55GXx4HrT1C1jCGYbzhW'), ToolMessage(content='Page: Ari Holtzman\\nSummary: Ari Holtzman is a professor of Computer Science at the University of Chicago and an expert in the area of Natural language processing and Computational linguistics. Previously, Holtzman was a PhD student at the University of Washington where he was advised by Luke Zettlemoyer.\\nIn 2017, he was a member of the winning team for the inaugural Alexa Prize for developing a conversational AI system for the Amazon Alexa device. Holtzman has made multiple contributions in the area of text generation and language models such as the introduction of nucleus sampling in 2019, his work on AI safety and neural fake news detection, and the fine-tuning of quantized large language models.\\n\\nPage: Top-p sampling\\nSummary: Top-p sampling, also called nucleus sampling, is a technique for autoregressive language model decoding proposed by Ari Holtzman et al. in 2019. Before the introduction of nucleus sampling, maximum likelihood decoding and beam search were the standard techniques for text generation, but, both of these decoding strategies are prone to generating texts that are repetitive and otherwise unnatural. Top-p sampling avoids this by setting a threshold p and then restricting the sampling to the set of most probable tokens with cumulative probability more than p. Then, probabilities of the token from this set are rescaled to sum up to 1, the rest of tokens are rejected.\\nTop-k sampling is similar except that the sample is taken from the k-highest probability tokens regardless of their cumulative probability. The advantage of top-p sampling is that one avoids the difficult problem of choosing the optimal value of k which can vary depending on the shape of the output distribution and the particular task and dataset.\\nThe top-p sampling technique is used in popular large language model applications and is implemented in language modeling frameworks like Hugging Face and Cohere.\\n\\n\\n\\nPage: List of American films of 2025\\nSummary: This is a list of American films that are scheduled to release in 2025.\\nFollowing the box office section, this list is organized chronologically, providing information on release dates, production companies, directors, and principal cast members.\\n\\n', name='wikipedia', id='1149ba85-4bc5-4518-924a-c115c553c00d', tool_call_id='call_USiSpwobQ4wL6U2dvtZtvbZL'), ToolMessage(content='Page: List of large language models\\nSummary: A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThis page lists notable large language models.\\n\\n\\n\\nPage: BERT (language model)\\nSummary: Bidirectional encoder representations from transformers (BERT) is a language model introduced in October 2018 by researchers at Google. It learns to represent text as a sequence of vectors using self-supervised learning. It uses the encoder-only transformer architecture. BERT dramatically improved the state-of-the-art for large language models. As of 2020, BERT is a ubiquitous baseline in natural language processing (NLP) experiments. \\nBERT is trained by masked token prediction and next sentence prediction. As a result of this training process, BERT learns contextual, latent representations of tokens in their context, similar to ELMo and GPT-2. It found applications for many natural language processing tasks, such as coreference resolution and polysemy resolution. It is an evolutionary step over ELMo, and spawned the study of \"BERTology\", which attempts to interpret what is learned by BERT.\\nBERT was originally implemented in the English language at two model sizes, BERTBASE (110 million parameters) and BERTLARGE (340 million parameters). Both were trained on the Toronto BookCorpus (800M words) and English Wikipedia  (2,500M words). The weights were released on GitHub. On March 11, 2020, 24 smaller models were released, the smallest being BERTTINY with just 4 million parameters.\\n\\nPage: Ari Holtzman\\nSummary: Ari Holtzman is a professor of Computer Science at the University of Chicago and an expert in the area of Natural language processing and Computational linguistics. Previously, Holtzman was a PhD student at the University of Washington where he was advised by Luke Zettlemoyer.\\nIn 2017, he was a member of the winning team for the inaugural Alexa Prize for developing a conversational AI system for the Amazon Alexa device. Holtzman has made multiple contributions in the area of text generation and language models such as the introduction of nucleus sampling in 2019, his work on AI safety and neural fake news detection, and the fine-tuning of quantized large language models.', name='wikipedia', id='51b5a994-2b3c-484f-9c06-362579a645c6', tool_call_id='call_J6khaFAxpGsraWHvLwud5p0y')]\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_cLXPBiToGJRMIOMLQav0q9o1', 'function': {'arguments': '{\"query\": \"Tim Dettmers latest tweet\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_a6d7W0w2dCwZtdfkQzU1LY4X', 'function': {'arguments': '{\"query\": \"Artidoro Pagnoni latest tweet\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_4EmdfgJEleKRiUaBDELz59JX', 'function': {'arguments': '{\"query\": \"Ari Holtzman latest tweet\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_6MO5GxMtLJP1AUMcI7Pte4wI', 'function': {'arguments': '{\"query\": \"Luke Zettlemoyer latest tweet\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 113, 'prompt_tokens': 3136, 'total_tokens': 3249, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 3072}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a6889ffe71', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-8415a8c2-d36c-4e8d-aa34-8bf1a81c06ed-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'Tim Dettmers latest tweet'}, 'id': 'call_cLXPBiToGJRMIOMLQav0q9o1', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'Artidoro Pagnoni latest tweet'}, 'id': 'call_a6d7W0w2dCwZtdfkQzU1LY4X', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'Ari Holtzman latest tweet'}, 'id': 'call_4EmdfgJEleKRiUaBDELz59JX', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'Luke Zettlemoyer latest tweet'}, 'id': 'call_6MO5GxMtLJP1AUMcI7Pte4wI', 'type': 'tool_call'}], usage_metadata={'input_tokens': 3136, 'output_tokens': 113, 'total_tokens': 3249, 'input_token_details': {'audio': 0, 'cache_read': 3072}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: tavily_search_results_json\n",
            "[ToolMessage(content='[{\"url\": \"https://threadreaderapp.com/user/Tim_Dettmers\", \"content\": \"Oct 8, 2021 • 13 tweets • 5 min read. I am excited to share my latest work: 8-bit optimizers – a replacement for regular optimizers. Faster , 75% less\"}, {\"url\": \"https://timdettmers.com/\", \"content\": \"Filed Under: Academia, PhD Life Tagged With: Advisors, Grad school, PhD\\\\n\\\\nTPUs vs GPUs for Transformers (BERT)\\\\n\\\\n2018-10-17 by Tim Dettmers 26 Comments [...] Filed Under: Deep Learning, Hardware Tagged With: AMD, CPU, High Performance Computing, Matrix Multiplication, Parallel Computing, PCIe Lanes, Sparse Training\\\\n\\\\nLLM.int8() and Emergent Features\\\\n\\\\n2022-08-17 by Tim Dettmers 13 Comments [...] [Read more…] about How to Choose Your Grad School\\\\n\\\\nFiled Under: Academia, PhD Life Tagged With: Advisors, Grad school, PhD\\\\n\\\\nOn Creativity in Academia\\\\n\\\\n2019-09-03 by Tim Dettmers 5 Comments\"}, {\"url\": \"https://twitter.com/Tim_Dettmers\", \"content\": \"Perplexity Deep Research is quite close to OpenAI o3 on the Humanity Last Exam Benchmark despite being an order of magnitude faster and cheaper. This is\"}, {\"url\": \"https://developer.nvidia.com/blog/author/tdettmers/\", \"content\": \"He runs a blog about deep learning and takes part in Kaggle data science competitions where he has reached a world rank of 63. Follow @Tim_Dettmers on Twitter\"}, {\"url\": \"https://threadreaderapp.com/thread/1666076553665744896.html\", \"content\": \"Make a small donation by buying us coffee ($5) or help with server cost ($10)\\\\n\\\\nOr Donate anonymously using crypto!\\\\n\\\\nEthereum\\\\n\\\\n0xfe58350B80634f60Fa6Dc149a72b4DFbc17D341E\\\\ncopy\\\\n\\\\nBitcoin\\\\n\\\\n3ATGMxNzCUFzxpMCHL5sWSt4DVtS8UqXpi\\\\ncopy\\\\n\\\\nThank you for your support!\\\\n\\\\nSend Email!\\\\n\\\\nEmail the whole thread instead of just a link! [...] More from @Tim_Dettmers\\\\n\\\\nTim Dettmers\\\\n\\\\nTim Dettmers\\\\n\\\\nTim Dettmers\\\\n\\\\nTim Dettmers\\\\n\\\\nTim Dettmers\\\\n\\\\nTim Dettmers\\\\n\\\\nDid Thread Reader help you today?\\\\n\\\\nSupport us! We are indie developers!\\\\n\\\\nThis site is made by just two indie developers on a laptop doing marketing, support and development! Read more about the story.\\\\n\\\\nBecome a Premium Member ($3/month or $30/year) and get exclusive features!\\\\n\\\\nBecome Premium\\\\n\\\\nDon\\'t want to be a Premium member but still want to support us?\"}]', name='tavily_search_results_json', id='f006028d-e710-43b5-9240-0f7b1fef7a53', tool_call_id='call_cLXPBiToGJRMIOMLQav0q9o1', artifact={'query': 'Tim Dettmers latest tweet', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://threadreaderapp.com/user/Tim_Dettmers', 'title': \"Tim Dettmers's Threads - Thread Reader App\", 'content': 'Oct 8, 2021 • 13 tweets • 5 min read. I am excited to share my latest work: 8-bit optimizers – a replacement for regular optimizers. Faster , 75% less', 'score': 0.73138535, 'raw_content': None}, {'url': 'https://timdettmers.com/', 'title': 'Tim Dettmers — Making deep learning accessible.', 'content': 'Filed Under: Academia, PhD Life Tagged With: Advisors, Grad school, PhD\\n\\nTPUs vs GPUs for Transformers (BERT)\\n\\n2018-10-17 by Tim Dettmers 26 Comments [...] Filed Under: Deep Learning, Hardware Tagged With: AMD, CPU, High Performance Computing, Matrix Multiplication, Parallel Computing, PCIe Lanes, Sparse Training\\n\\nLLM.int8() and Emergent Features\\n\\n2022-08-17 by Tim Dettmers 13 Comments [...] [Read more…] about How to Choose Your Grad School\\n\\nFiled Under: Academia, PhD Life Tagged With: Advisors, Grad school, PhD\\n\\nOn Creativity in Academia\\n\\n2019-09-03 by Tim Dettmers 5 Comments', 'score': 0.6649108, 'raw_content': None}, {'url': 'https://twitter.com/Tim_Dettmers', 'title': 'Tim Dettmers (@Tim_Dettmers) / X', 'content': 'Perplexity Deep Research is quite close to OpenAI o3 on the Humanity Last Exam Benchmark despite being an order of magnitude faster and cheaper. This is', 'score': 0.6147415, 'raw_content': None}, {'url': 'https://developer.nvidia.com/blog/author/tdettmers/', 'title': 'Author: Tim Dettmers | NVIDIA Technical Blog', 'content': 'He runs a blog about deep learning and takes part in Kaggle data science competitions where he has reached a world rank of 63. Follow @Tim_Dettmers on Twitter', 'score': 0.58948064, 'raw_content': None}, {'url': 'https://threadreaderapp.com/thread/1666076553665744896.html', 'title': 'Thread by @Tim_Dettmers on Thread Reader App', 'content': \"Make a small donation by buying us coffee ($5) or help with server cost ($10)\\n\\nOr Donate anonymously using crypto!\\n\\nEthereum\\n\\n0xfe58350B80634f60Fa6Dc149a72b4DFbc17D341E\\ncopy\\n\\nBitcoin\\n\\n3ATGMxNzCUFzxpMCHL5sWSt4DVtS8UqXpi\\ncopy\\n\\nThank you for your support!\\n\\nSend Email!\\n\\nEmail the whole thread instead of just a link! [...] More from @Tim_Dettmers\\n\\nTim Dettmers\\n\\nTim Dettmers\\n\\nTim Dettmers\\n\\nTim Dettmers\\n\\nTim Dettmers\\n\\nTim Dettmers\\n\\nDid Thread Reader help you today?\\n\\nSupport us! We are indie developers!\\n\\nThis site is made by just two indie developers on a laptop doing marketing, support and development! Read more about the story.\\n\\nBecome a Premium Member ($3/month or $30/year) and get exclusive features!\\n\\nBecome Premium\\n\\nDon't want to be a Premium member but still want to support us?\", 'score': 0.5492786, 'raw_content': None}], 'response_time': 2.93}), ToolMessage(content='[{\"url\": \"https://artidoro.github.io/\", \"content\": \"I have recently developed the Byte Latent Transformer, a new architecture that efficiently learns from raw byte data unlocking a new scaling dimension and paving the path towards universal byte models. With QLoRA, I reduced finetuning memory requirements by 15x and showed how to approach ChatGPT 3.5 performance in 24h on a single GPU. [...] Previously, I have also worked on sythetic data augmentation for improved controllability of generation systems, investigated language models’ reasoning and world modeling abilities, and evaluated their factual errors, as well as societal challenge associated with their use. [...] Artidoro Pagnoni\\\\n\\\\nPhD student in NLP at the University of Washington\\\\n\\\\nCiao!\\\\n\\\\nI am a final-year PhD student in Computer Science at the University of Washington, advised by Luke Zettlemoyer, and a visiting researcher at Meta. My research focuses on resource efficiency and improving LLM scaling trends.\"}, {\"url\": \"https://twitter.com/ArtidoroPagnoni/highlights\", \"content\": \"Artidoro Pagnoni\\'s Highlights ... Introducing the Byte Latent Transformer (BLT) – An LLM architecture that scales better than Llama 3 using byte-patches instead\"}, {\"url\": \"https://x.com/artidoropagnoni?lang=en\", \"content\": \"https://huggingface.co/facebook/blt Hoping to see many new and improved BLT based architectures this year!\"}, {\"url\": \"https://twitter.com/apagnonipagnoni?lang=ms\", \"content\": \"Don\\'t let climate change deniers derail progress in the global effort to #ActOnClimate: http://ofa.bo/e5FW. Imej. 204. 565. 1K. Artidoro Pagnoni menyiarkan\"}, {\"url\": \"https://x.com/ArtidoroPagnoni/status/1867601413741981804\", \"content\": \"Artidoro Pagnoni · @ArtidoroPagnoni. Introducing the Byte Latent Transformer (BLT) – An LLM architecture that scales better than Llama 3\"}]', name='tavily_search_results_json', id='678d7734-0774-4d24-b93d-06c2791f6dfc', tool_call_id='call_a6d7W0w2dCwZtdfkQzU1LY4X', artifact={'query': 'Artidoro Pagnoni latest tweet', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://artidoro.github.io/', 'title': 'Artidoro Pagnoni: Ciao!', 'content': 'I have recently developed the Byte Latent Transformer, a new architecture that efficiently learns from raw byte data unlocking a new scaling dimension and paving the path towards universal byte models. With QLoRA, I reduced finetuning memory requirements by 15x and showed how to approach ChatGPT 3.5 performance in 24h on a single GPU. [...] Previously, I have also worked on sythetic data augmentation for improved controllability of generation systems, investigated language models’ reasoning and world modeling abilities, and evaluated their factual errors, as well as societal challenge associated with their use. [...] Artidoro Pagnoni\\n\\nPhD student in NLP at the University of Washington\\n\\nCiao!\\n\\nI am a final-year PhD student in Computer Science at the University of Washington, advised by Luke Zettlemoyer, and a visiting researcher at Meta. My research focuses on resource efficiency and improving LLM scaling trends.', 'score': 0.64746445, 'raw_content': None}, {'url': 'https://twitter.com/ArtidoroPagnoni/highlights', 'title': 'Highlights by Artidoro Pagnoni (@ArtidoroPagnoni) / X', 'content': \"Artidoro Pagnoni's Highlights ... Introducing the Byte Latent Transformer (BLT) – An LLM architecture that scales better than Llama 3 using byte-patches instead\", 'score': 0.64719695, 'raw_content': None}, {'url': 'https://x.com/artidoropagnoni?lang=en', 'title': 'Artidoro Pagnoni (@ArtidoroPagnoni) / X', 'content': 'https://huggingface.co/facebook/blt Hoping to see many new and improved BLT based architectures this year!', 'score': 0.6347929, 'raw_content': None}, {'url': 'https://twitter.com/apagnonipagnoni?lang=ms', 'title': 'Artidoro Pagnoni (@ApagnoniPagnoni) / X', 'content': \"Don't let climate change deniers derail progress in the global effort to #ActOnClimate: http://ofa.bo/e5FW. Imej. 204. 565. 1K. Artidoro Pagnoni menyiarkan\", 'score': 0.61168414, 'raw_content': None}, {'url': 'https://x.com/ArtidoroPagnoni/status/1867601413741981804', 'title': 'Artidoro Pagnoni - X', 'content': 'Artidoro Pagnoni · @ArtidoroPagnoni. Introducing the Byte Latent Transformer (BLT) – An LLM architecture that scales better than Llama 3', 'score': 0.60274047, 'raw_content': None}], 'response_time': 3.94}), ToolMessage(content='[{\"url\": \"https://x.com/universeinanegg?lang=en\", \"content\": \"I\\'m starting Conceptualization Lab, recruiting PhDs & Postdocs! We need new abstractions to understand LLMs. Conceptualization is the act of building\"}, {\"url\": \"https://x.com/aaholtzman?lang=en\", \"content\": \"In other news, I\\'ll be joining @UChicagoCS and @DSI_UChicago in 2024 as an assistant professor and doing a postdoc @Meta in the meantime! I\\'m at\"}, {\"url\": \"https://twitter.com/universeinanegg/status/1848794897773650368\", \"content\": \"UChicago has a fast growing Communication & Intelligence community, that @ChenhaoTan and @MinaLee__ co-founded along with me.\"}, {\"url\": \"https://aclanthology.org/2024.lrec-main.986.pdf\", \"content\": \"Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. In Advances in Neural Information Processing Systems, volume 32. Curran Asso-ciates, Inc.\\\\nXinyi Zhou and Reza Zafarani. 2020. A survey of fake news: Fundamental theories, detection methods, and opportunities. ACM Computing Surveys (CSUR), 53(5):1–40. [...] Query-2 arma 3 russia ukraine lang:en -is:retweet 2021-12-21 2022-04-06 100 Query-3 for Other russia ukraine war video lang:en (has:media OR has:geo) -is:retweet 2021-12-21 2022-04-06 50 Query-4 for Other russia ukraine war video lang:en -is:retweet 2021-12-21 2022-04-06 50 Table 6: An example event in the dataset. A part of URL is cropped due to space constraints. [...] 2022-04 2022-05 0.000 0.001 0.002 0.003 0.004 0.005 0.006 Density EN-Covid Event EN12 EN13 EN14 EN15 EN16 EN17 EN18 EN19 EN20 (b) COVID-19 in English 2020-09 2020-11 2021-01 2021-03 2021-05 2021-07 2021-09 2021-11 2022-01 2022-03 2022-05 0.00 0.02 0.04 0.06 0.08 0.10 Density EN-Immigration EN21 EN22 EN23 EN24 EN25 EN26 EN27 EN28 EN29 EN30 (c) Immigration in English 2022-02-01 2022-02-15 2022-03-01 2022-03-15 2022-04-01 2022-04-15 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 Density TR-Ukraine TR01\"}, {\"url\": \"https://twitter.com/universeinanegg/status/1764113353113321929\", \"content\": \"My biggest challenge in life has consistently been: what skills that I don\\'t have a natural talent for are worth honing?\"}]', name='tavily_search_results_json', id='84642bc2-dfc3-4cca-ac0a-7e9952414d5a', tool_call_id='call_4EmdfgJEleKRiUaBDELz59JX', artifact={'query': 'Ari Holtzman latest tweet', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://x.com/universeinanegg?lang=en', 'title': 'Ari Holtzman (@universeinanegg) / X', 'content': \"I'm starting Conceptualization Lab, recruiting PhDs & Postdocs! We need new abstractions to understand LLMs. Conceptualization is the act of building\", 'score': 0.6601948, 'raw_content': None}, {'url': 'https://x.com/aaholtzman?lang=en', 'title': 'Ari Holtzman (@aaholtzman) / X', 'content': \"In other news, I'll be joining @UChicagoCS and @DSI_UChicago in 2024 as an assistant professor and doing a postdoc @Meta in the meantime! I'm at\", 'score': 0.64478505, 'raw_content': None}, {'url': 'https://twitter.com/universeinanegg/status/1848794897773650368', 'title': 'Ari Holtzman on X: \"UChicago has a fast growing Communication ...', 'content': 'UChicago has a fast growing Communication & Intelligence community, that @ChenhaoTan and @MinaLee__ co-founded along with me.', 'score': 0.5917474, 'raw_content': None}, {'url': 'https://aclanthology.org/2024.lrec-main.986.pdf', 'title': '[PDF] An Annotated Multi-Event Tweet Dataset for Misinformation Detection', 'content': 'Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. In Advances in Neural Information Processing Systems, volume 32. Curran Asso-ciates, Inc.\\nXinyi Zhou and Reza Zafarani. 2020. A survey of fake news: Fundamental theories, detection methods, and opportunities. ACM Computing Surveys (CSUR), 53(5):1–40. [...] Query-2 arma 3 russia ukraine lang:en -is:retweet 2021-12-21 2022-04-06 100 Query-3 for Other russia ukraine war video lang:en (has:media OR has:geo) -is:retweet 2021-12-21 2022-04-06 50 Query-4 for Other russia ukraine war video lang:en -is:retweet 2021-12-21 2022-04-06 50 Table 6: An example event in the dataset. A part of URL is cropped due to space constraints. [...] 2022-04 2022-05 0.000 0.001 0.002 0.003 0.004 0.005 0.006 Density EN-Covid Event EN12 EN13 EN14 EN15 EN16 EN17 EN18 EN19 EN20 (b) COVID-19 in English 2020-09 2020-11 2021-01 2021-03 2021-05 2021-07 2021-09 2021-11 2022-01 2022-03 2022-05 0.00 0.02 0.04 0.06 0.08 0.10 Density EN-Immigration EN21 EN22 EN23 EN24 EN25 EN26 EN27 EN28 EN29 EN30 (c) Immigration in English 2022-02-01 2022-02-15 2022-03-01 2022-03-15 2022-04-01 2022-04-15 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 Density TR-Ukraine TR01', 'score': 0.52597815, 'raw_content': None}, {'url': 'https://twitter.com/universeinanegg/status/1764113353113321929', 'title': 'Ari Holtzman on X: \"My biggest challenge in life has consistently ...', 'content': \"My biggest challenge in life has consistently been: what skills that I don't have a natural talent for are worth honing?\", 'score': 0.48264915, 'raw_content': None}], 'response_time': 2.24}), ToolMessage(content='[{\"url\": \"https://twitter.com/LukeZettlemoyer\", \"content\": \"Our latest work uses multi-agent reinforcement learning to model corporate investment in climate change mitigation as a social dilemma. We create a new\"}, {\"url\": \"https://twitter.com/lukezettlemoyer/status/1003662931479941120\", \"content\": \"Come see Julian Michael presenting his work in question answer meaning representations, now at @NAACLHLT ! Image. 3:41 PM · Jun 4, 2018.\"}, {\"url\": \"https://x.com/lukezettlemoyer?lang=en\", \"content\": \"New paper from FAIR+NYU: Q: Is language supervision required to learn effective visual representations for multimodal tasks? A: No.\"}, {\"url\": \"https://openreview.net/pdf/1cb1687474929f19b009c09c143eca5e9724e2af.pdf\", \"content\": \"691 In Proceedings of the 2018 Conference on Empiri-692 cal Methods in Natural Language Processing, pages 693 659–664, Brussels, Belgium. Association for Com-694 putational Linguistics.\\\\n695 Mike Lewis, Yinhan Liu, Naman Goyal, Marjan 696 Ghazvininejad, Abdelrahman Mohamed, Omer Levy, 697 Veselin Stoyanov, and Luke Zettlemoyer. 2020. [...] Length 26.08 30.70 44.24 49.52 Verb 3.80 4.48 5.95 8.31 Noun 8.05 9.57 13.90 16.13 Adjective 1.82 2.14 3.23 4.15 Hashtag 1.04 1.16 3.10 1.76 Positive 1.42 1.63 2.90 3.68 Negative 1.06 1.33 1.91 1.66 Generality.\\\\nIndef 0.54 0.67 0.89 1.30 Def 1.13 1.27 1.78 2.27 Readability.\\\\nFlesch Score ↑ 44.71 48.75 23.07 24.71 Flesch Level ↓ 13.79 12.12 18.75 14.84 Imitation. [...] LLaMA2-Chat: Pondering the future of #customersupport: Will #ChatGPT be the silver bullet for orgs? #AI #language-model ChatGPT: Revolutionizing Customer Support with Chat-GPT! Discover how language models like ChatGPT are not just conversing and blogging, but also acing legal tests. Is this the future of customer service? #ChatGPT #CustomerServi-ceInnovation #AIRevolution RaPALM: Just set up my ChatGPT and I’m blown away by its capabilities! Just learn and try it on your customer support team.\"}, {\"url\": \"https://news.cs.washington.edu/2017/10/22/luke-zettlemoyer-allen-institute-for-ai-in-ny-times/\", \"content\": \"Allen School News\\\\n\\\\nLuke Zettlemoyer, Allen Institute for AI in NY Times\\\\n\\\\n“Luke Zettlemoyer, a professor at the University of Washington … turned down a lucrative offer from Google, instead taking a post at the nonprofit Allen Institute for Artificial Intelligence so he could continue teaching.”\\\\n\\\\nLuke and Ali Farhadi are heavily engaged in AI2, which is led by long-time Allen School professor Oren Etzioni. It offers the best of both worlds.\\\\n\\\\nNY Times article here.\\\\n\\\\nConnect With #UWAllen [...] Undergraduate Advising: ugrad-adviser at cs.washington.edu\\\\n                            Professional Master\\'s Program Advising: masters at cs.washington.edu\\\\n                            Graduate (Ph.D.) Advising: grad-advising at cs.washington.edu\\\\n\\\\nCopyright © 2017-2021 University of Washington - Seattle -\\\\n                        Paul G. Allen School of Computer Science & Engineering | All Rights |\\\\n                        Privacy |\\\\n                        Terms [...] Contact us:\\\\n                            Main Tel: (206) 543-1695\\\\n                            Media Inquiries: media at cs.washington.edu\\\\n                            Webmaster: support at cs.washington.edu\\\\n\\\\nComputer Engineering degree program accredited by ABET\"}]', name='tavily_search_results_json', id='95996f18-1777-4de4-8602-bed13b06626c', tool_call_id='call_6MO5GxMtLJP1AUMcI7Pte4wI', artifact={'query': 'Luke Zettlemoyer latest tweet', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://twitter.com/LukeZettlemoyer', 'title': 'Luke Zettlemoyer (@LukeZettlemoyer) / X', 'content': 'Our latest work uses multi-agent reinforcement learning to model corporate investment in climate change mitigation as a social dilemma. We create a new', 'score': 0.6721816, 'raw_content': None}, {'url': 'https://twitter.com/lukezettlemoyer/status/1003662931479941120', 'title': 'Luke Zettlemoyer on X: \"Come see Julian Michael presenting his ...', 'content': 'Come see Julian Michael presenting his work in question answer meaning representations, now at @NAACLHLT ! Image. 3:41 PM · Jun 4, 2018.', 'score': 0.6147415, 'raw_content': None}, {'url': 'https://x.com/lukezettlemoyer?lang=en', 'title': 'Luke Zettlemoyer (@LukeZettlemoyer) / X', 'content': 'New paper from FAIR+NYU: Q: Is language supervision required to learn effective visual representations for multimodal tasks? A: No.', 'score': 0.5911811, 'raw_content': None}, {'url': 'https://openreview.net/pdf/1cb1687474929f19b009c09c143eca5e9724e2af.pdf', 'title': '[PDF] Popular Quoting Tweet Generation via Auto ... - OpenReview', 'content': '691 In Proceedings of the 2018 Conference on Empiri-692 cal Methods in Natural Language Processing, pages 693 659–664, Brussels, Belgium. Association for Com-694 putational Linguistics.\\n695 Mike Lewis, Yinhan Liu, Naman Goyal, Marjan 696 Ghazvininejad, Abdelrahman Mohamed, Omer Levy, 697 Veselin Stoyanov, and Luke Zettlemoyer. 2020. [...] Length 26.08 30.70 44.24 49.52 Verb 3.80 4.48 5.95 8.31 Noun 8.05 9.57 13.90 16.13 Adjective 1.82 2.14 3.23 4.15 Hashtag 1.04 1.16 3.10 1.76 Positive 1.42 1.63 2.90 3.68 Negative 1.06 1.33 1.91 1.66 Generality.\\nIndef 0.54 0.67 0.89 1.30 Def 1.13 1.27 1.78 2.27 Readability.\\nFlesch Score ↑ 44.71 48.75 23.07 24.71 Flesch Level ↓ 13.79 12.12 18.75 14.84 Imitation. [...] LLaMA2-Chat: Pondering the future of #customersupport: Will #ChatGPT be the silver bullet for orgs? #AI #language-model ChatGPT: Revolutionizing Customer Support with Chat-GPT! Discover how language models like ChatGPT are not just conversing and blogging, but also acing legal tests. Is this the future of customer service? #ChatGPT #CustomerServi-ceInnovation #AIRevolution RaPALM: Just set up my ChatGPT and I’m blown away by its capabilities! Just learn and try it on your customer support team.', 'score': 0.48982134, 'raw_content': None}, {'url': 'https://news.cs.washington.edu/2017/10/22/luke-zettlemoyer-allen-institute-for-ai-in-ny-times/', 'title': 'Luke Zettlemoyer, Allen Institute for AI in NY Times', 'content': \"Allen School News\\n\\nLuke Zettlemoyer, Allen Institute for AI in NY Times\\n\\n“Luke Zettlemoyer, a professor at the University of Washington … turned down a lucrative offer from Google, instead taking a post at the nonprofit Allen Institute for Artificial Intelligence so he could continue teaching.”\\n\\nLuke and Ali Farhadi are heavily engaged in AI2, which is led by long-time Allen School professor Oren Etzioni. It offers the best of both worlds.\\n\\nNY Times article here.\\n\\nConnect With #UWAllen [...] Undergraduate Advising: ugrad-adviser at cs.washington.edu\\n                            Professional Master's Program Advising: masters at cs.washington.edu\\n                            Graduate (Ph.D.) Advising: grad-advising at cs.washington.edu\\n\\nCopyright © 2017-2021 University of Washington - Seattle -\\n                        Paul G. Allen School of Computer Science & Engineering | All Rights |\\n                        Privacy |\\n                        Terms [...] Contact us:\\n                            Main Tel: (206) 543-1695\\n                            Media Inquiries: media at cs.washington.edu\\n                            Webmaster: support at cs.washington.edu\\n\\nComputer Engineering degree program accredited by ABET\", 'score': 0.35307175, 'raw_content': None}], 'response_time': 2.8})]\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content=\"Here are the findings for the authors of the QLoRA paper:\\n\\n1. **Tim Dettmers**:\\n   - **Wikipedia**: No dedicated Wikipedia page found.\\n   - **Latest Tweet**: Tim Dettmers tweeted about Perplexity Deep Research being close to OpenAI o3 on the Humanity Last Exam Benchmark, highlighting its speed and cost efficiency. [Source](https://twitter.com/Tim_Dettmers).\\n\\n2. **Artidoro Pagnoni**:\\n   - **Wikipedia**: No dedicated Wikipedia page found.\\n   - **Latest Tweet**: Artidoro Pagnoni introduced the Byte Latent Transformer (BLT), an LLM architecture that scales better than Llama 3. [Source](https://x.com/ArtidoroPagnoni/status/1867601413741981804).\\n\\n3. **Ari Holtzman**:\\n   - **Wikipedia**: Ari Holtzman is a professor of Computer Science at the University of Chicago, known for his work in natural language processing and computational linguistics.\\n   - **Latest Tweet**: Ari Holtzman tweeted about the challenge of honing skills that don't come naturally. [Source](https://twitter.com/universeinanegg/status/1764113353113321929).\\n\\n4. **Luke Zettlemoyer**:\\n   - **Wikipedia**: No dedicated Wikipedia page found.\\n   - **Latest Tweet**: Luke Zettlemoyer discussed a new paper from FAIR+NYU about language supervision and visual representations for multimodal tasks. [Source](https://x.com/lukezettlemoyer?lang=en).\\n\\nIf you need more detailed information or further assistance, feel free to ask!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 347, 'prompt_tokens': 6067, 'total_tokens': 6414, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 3840}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a6889ffe71', 'finish_reason': 'stop', 'logprobs': None}, id='run-acf04847-f847-4d63-bad0-e17f47740e6e-0', usage_metadata={'input_tokens': 6067, 'output_tokens': 347, 'total_tokens': 6414, 'input_token_details': {'audio': 0, 'cache_read': 3840}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "=== History Trace ===\n",
            "\n",
            "1. Step 1: Agent called model.\n",
            "2. Step 1: Tool executed.\n",
            "3. Step 2: Agent called model.\n",
            "4. Step 2: Tool executed.\n",
            "5. Step 3: Agent called model.\n",
            "6. Step 3: Tool executed.\n",
            "7. Step 4: Agent called model.\n",
            "8. Step 4: No tool call. Ending.\n"
          ]
        }
      ],
      "source": [
        "# Restricted to a steps_taken of 2 or fewer.... The LLM doesnt complete the task and exits the loop mid way.\n",
        "# inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the QLoRA paper, identify the authors and find if they have any Wikipedia pages, then search each of the authors to find out their latest Tweet using Tavily!\")]}\n",
        "\n",
        "# async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "#     for node, values in chunk.items():\n",
        "#         print(f\"Receiving update from node: '{node}'\")\n",
        "#         if node == \"action\":\n",
        "#           print(f\"Tool Used: {values['messages'][0].name}\")\n",
        "#         print(values[\"messages\"])\n",
        "\n",
        "#         print(\"\\n\\n\")\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\n",
        "    \"messages\": [\n",
        "        HumanMessage(\n",
        "            content=\"Search Arxiv for the QLoRA paper, identify the authors and find if they have any Wikipedia pages, then search each of the authors to find out their latest Tweet using Tavily!\"\n",
        "        )\n",
        "    ],\n",
        "    \"steps_taken\": 0,\n",
        "    \"history\": []\n",
        "}\n",
        "\n",
        "# 1. Stream the execution\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "\n",
        "        if node == \"action\":\n",
        "            tool_call = values[\"messages\"][0]\n",
        "            print(f\"Tool Used: {tool_call.name}\")\n",
        "\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\")\n",
        "\n",
        "# 2. Run again to fetch full final state (includes `history`)\n",
        "final_state = await compiled_graph.ainvoke(inputs)\n",
        "\n",
        "# 3. Display history\n",
        "print(\"\\n=== History Trace ===\\n\")\n",
        "for i, step in enumerate(final_state.get(\"history\", []), 1):\n",
        "    print(f\"{i}. {step}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_R8gByxxHhW0e92vqktpkVVv6', 'function': {'arguments': '{\"query\":\"QLoRA\"}', 'name': 'arxiv'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 248, 'total_tokens': 265, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a6889ffe71', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-5091bec2-1819-471c-8bb7-29594ece8cca-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'QLoRA'}, 'id': 'call_R8gByxxHhW0e92vqktpkVVv6', 'type': 'tool_call'}], usage_metadata={'input_tokens': 248, 'output_tokens': 17, 'total_tokens': 265, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: arxiv\n",
            "[ToolMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2025-02-05\\nTitle: Resource-Efficient & Effective Code Summarization\\nAuthors: Saima Afrin, Joseph Call, Khai-Nguyen Nguyen, Oscar Chaparro, Antonio Mastropaolo\\nSummary: Code Language Models (CLMs) have demonstrated high effectiveness in\\nautomating software engineering tasks such as bug fixing, code generation, and\\ncode documentation. This ', name='arxiv', id='f24d9baf-1f7c-4bab-9958-85e9ce32f2f2', tool_call_id='call_R8gByxxHhW0e92vqktpkVVv6')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_iTZwVbUseSYDTdevMBomR4SA', 'function': {'arguments': '{\"query\": \"Tim Dettmers\"}', 'name': 'wikipedia'}, 'type': 'function'}, {'id': 'call_sKgbUF1LDVNxSf6ezZOKK6Fg', 'function': {'arguments': '{\"query\": \"Artidoro Pagnoni\"}', 'name': 'wikipedia'}, 'type': 'function'}, {'id': 'call_RUrffN4QnpHmnMeMRbm8XTqE', 'function': {'arguments': '{\"query\": \"Ari Holtzman\"}', 'name': 'wikipedia'}, 'type': 'function'}, {'id': 'call_VrhGVN6LG4DOfEhWI2zsEkQj', 'function': {'arguments': '{\"query\": \"Luke Zettlemoyer\"}', 'name': 'wikipedia'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 85, 'prompt_tokens': 1218, 'total_tokens': 1303, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1152}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a6889ffe71', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-820a6182-e879-47ea-9e40-73f1a0d82df7-0', tool_calls=[{'name': 'wikipedia', 'args': {'query': 'Tim Dettmers'}, 'id': 'call_iTZwVbUseSYDTdevMBomR4SA', 'type': 'tool_call'}, {'name': 'wikipedia', 'args': {'query': 'Artidoro Pagnoni'}, 'id': 'call_sKgbUF1LDVNxSf6ezZOKK6Fg', 'type': 'tool_call'}, {'name': 'wikipedia', 'args': {'query': 'Ari Holtzman'}, 'id': 'call_RUrffN4QnpHmnMeMRbm8XTqE', 'type': 'tool_call'}, {'name': 'wikipedia', 'args': {'query': 'Luke Zettlemoyer'}, 'id': 'call_VrhGVN6LG4DOfEhWI2zsEkQj', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1218, 'output_tokens': 85, 'total_tokens': 1303, 'input_token_details': {'audio': 0, 'cache_read': 1152}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: wikipedia\n",
            "[ToolMessage(content='Page: Data parallelism\\nSummary: Data parallelism is parallelization across multiple processors in parallel computing environments. It focuses on distributing the data across different nodes, which operate on the data in parallel. It can be applied on regular data structures like arrays and matrices by working on each element in parallel. It contrasts to task parallelism as another form of parallelism.\\nA data parallel job on an array of n elements can be divided equally among all the processors. Let us assume we want to sum all the elements of the given array and the time for a single addition operation is Ta time units. In the case of sequential execution, the time taken by the process will be n×Ta time units as it sums up all the elements of an array. On the other hand, if we execute this job as a data parallel job on 4 processors the time taken would reduce to (n/4)×Ta + merging overhead time units. Parallel execution results in a speedup of 4 over sequential execution. The locality of data references plays an important part in evaluating the performance of a data parallel programming model. Locality of data depends on the memory accesses performed by the program as well as the size of the cache.\\n\\nPage: Ari Holtzman\\nSummary: Ari Holtzman is a professor of Computer Science at the University of Chicago and an expert in the area of Natural language processing and Computational linguistics. Previously, Holtzman was a PhD student at the University of Washington where he was advised by Luke Zettlemoyer.\\nIn 2017, he was a member of the winning team for the inaugural Alexa Prize for developing a conversational AI system for the Amazon Alexa device. Holtzman has made multiple contributions in the area of text generation and language models such as the introduction of nucleus sampling in 2019, his work on AI safety and neural fake news detection, and the fine-tuning of quantized large language models.\\n\\nPage: Bob Dettmer\\nSummary: Robert P. \"Bob\" Dettmer (born March 1, 1951) is a Minnesota politician and former member of the Minnesota House of Representatives. A member of the Republican Party of Minnesota, he represented District 39A, located in the northeastern part of the Twin Cities metro area, which includes Chisago and Washington counties.\\n\\n', name='wikipedia', id='0ea3b34f-c5bf-4d3c-ae9f-2121cad2f24c', tool_call_id='call_iTZwVbUseSYDTdevMBomR4SA'), ToolMessage(content='Page: Ari Holtzman\\nSummary: Ari Holtzman is a professor of Computer Science at the University of Chicago and an expert in the area of Natural language processing and Computational linguistics. Previously, Holtzman was a PhD student at the University of Washington where he was advised by Luke Zettlemoyer.\\nIn 2017, he was a member of the winning team for the inaugural Alexa Prize for developing a conversational AI system for the Amazon Alexa device. Holtzman has made multiple contributions in the area of text generation and language models such as the introduction of nucleus sampling in 2019, his work on AI safety and neural fake news detection, and the fine-tuning of quantized large language models.\\n\\nPage: Large language model\\nSummary: A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\\n\\nPage: ML.NET\\nSummary: ML.NET is a free software machine learning library for the C# and F# programming languages. It also supports Python models when used together with NimbusML. The preview release of ML.NET included transforms for feature engineering like n-gram creation, and learners to handle binary classification, multi-class classification, and regression tasks. Additional ML tasks like anomaly detection and recommendation systems have since been added, and other approaches like deep learning will be included in future versions.\\n\\n', name='wikipedia', id='1c4bf1e3-49c9-4d53-ba32-a138b63bab6a', tool_call_id='call_sKgbUF1LDVNxSf6ezZOKK6Fg'), ToolMessage(content='Page: Ari Holtzman\\nSummary: Ari Holtzman is a professor of Computer Science at the University of Chicago and an expert in the area of Natural language processing and Computational linguistics. Previously, Holtzman was a PhD student at the University of Washington where he was advised by Luke Zettlemoyer.\\nIn 2017, he was a member of the winning team for the inaugural Alexa Prize for developing a conversational AI system for the Amazon Alexa device. Holtzman has made multiple contributions in the area of text generation and language models such as the introduction of nucleus sampling in 2019, his work on AI safety and neural fake news detection, and the fine-tuning of quantized large language models.\\n\\nPage: Top-p sampling\\nSummary: Top-p sampling, also called nucleus sampling, is a technique for autoregressive language model decoding proposed by Ari Holtzman et al. in 2019. Before the introduction of nucleus sampling, maximum likelihood decoding and beam search were the standard techniques for text generation, but, both of these decoding strategies are prone to generating texts that are repetitive and otherwise unnatural. Top-p sampling avoids this by setting a threshold p and then restricting the sampling to the set of most probable tokens with cumulative probability more than p. Then, probabilities of the token from this set are rescaled to sum up to 1, the rest of tokens are rejected.\\nTop-k sampling is similar except that the sample is taken from the k-highest probability tokens regardless of their cumulative probability. The advantage of top-p sampling is that one avoids the difficult problem of choosing the optimal value of k which can vary depending on the shape of the output distribution and the particular task and dataset.\\nThe top-p sampling technique is used in popular large language model applications and is implemented in language modeling frameworks like Hugging Face and Cohere.\\n\\n\\n\\nPage: List of American films of 2025\\nSummary: This is a list of American films that are scheduled to release in 2025.\\nFollowing the box office section, this list is organized chronologically, providing information on release dates, production companies, directors, and principal cast members.\\n\\n', name='wikipedia', id='b7fbf719-40fc-49c1-a0d5-bc90d0824b25', tool_call_id='call_RUrffN4QnpHmnMeMRbm8XTqE'), ToolMessage(content='Page: List of large language models\\nSummary: A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThis page lists notable large language models.\\n\\n\\n\\nPage: BERT (language model)\\nSummary: Bidirectional encoder representations from transformers (BERT) is a language model introduced in October 2018 by researchers at Google. It learns to represent text as a sequence of vectors using self-supervised learning. It uses the encoder-only transformer architecture. BERT dramatically improved the state-of-the-art for large language models. As of 2020, BERT is a ubiquitous baseline in natural language processing (NLP) experiments. \\nBERT is trained by masked token prediction and next sentence prediction. As a result of this training process, BERT learns contextual, latent representations of tokens in their context, similar to ELMo and GPT-2. It found applications for many natural language processing tasks, such as coreference resolution and polysemy resolution. It is an evolutionary step over ELMo, and spawned the study of \"BERTology\", which attempts to interpret what is learned by BERT.\\nBERT was originally implemented in the English language at two model sizes, BERTBASE (110 million parameters) and BERTLARGE (340 million parameters). Both were trained on the Toronto BookCorpus (800M words) and English Wikipedia  (2,500M words). The weights were released on GitHub. On March 11, 2020, 24 smaller models were released, the smallest being BERTTINY with just 4 million parameters.\\n\\nPage: Ari Holtzman\\nSummary: Ari Holtzman is a professor of Computer Science at the University of Chicago and an expert in the area of Natural language processing and Computational linguistics. Previously, Holtzman was a PhD student at the University of Washington where he was advised by Luke Zettlemoyer.\\nIn 2017, he was a member of the winning team for the inaugural Alexa Prize for developing a conversational AI system for the Amazon Alexa device. Holtzman has made multiple contributions in the area of text generation and language models such as the introduction of nucleus sampling in 2019, his work on AI safety and neural fake news detection, and the fine-tuning of quantized large language models.', name='wikipedia', id='d0584e23-642b-4f9c-8fde-9eeed12ea4d2', tool_call_id='call_VrhGVN6LG4DOfEhWI2zsEkQj')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_SnURBwhO1j5LRdld5226x3pb', 'function': {'arguments': '{\"query\": \"Tim Dettmers latest tweet\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_SlKHQQNCyr0C76eRnYpVLWCS', 'function': {'arguments': '{\"query\": \"Artidoro Pagnoni latest tweet\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_2ByCmkQzV7Qo9ApQb58yUUeQ', 'function': {'arguments': '{\"query\": \"Ari Holtzman latest tweet\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_v1fOigWeGA30JwuiI1BpEzKP', 'function': {'arguments': '{\"query\": \"Luke Zettlemoyer latest tweet\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 113, 'prompt_tokens': 3136, 'total_tokens': 3249, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1280}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a6889ffe71', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-7c58a219-66d6-4207-8458-542f438cd1ed-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'Tim Dettmers latest tweet'}, 'id': 'call_SnURBwhO1j5LRdld5226x3pb', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'Artidoro Pagnoni latest tweet'}, 'id': 'call_SlKHQQNCyr0C76eRnYpVLWCS', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'Ari Holtzman latest tweet'}, 'id': 'call_2ByCmkQzV7Qo9ApQb58yUUeQ', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'Luke Zettlemoyer latest tweet'}, 'id': 'call_v1fOigWeGA30JwuiI1BpEzKP', 'type': 'tool_call'}], usage_metadata={'input_tokens': 3136, 'output_tokens': 113, 'total_tokens': 3249, 'input_token_details': {'audio': 0, 'cache_read': 1280}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: tavily_search_results_json\n",
            "[ToolMessage(content='[{\"url\": \"https://threadreaderapp.com/user/Tim_Dettmers\", \"content\": \"Oct 8, 2021 • 13 tweets • 5 min read. I am excited to share my latest work: 8-bit optimizers – a replacement for regular optimizers. Faster , 75% less\"}, {\"url\": \"https://timdettmers.com/\", \"content\": \"Filed Under: Academia, PhD Life Tagged With: Advisors, Grad school, PhD\\\\n\\\\nTPUs vs GPUs for Transformers (BERT)\\\\n\\\\n2018-10-17 by Tim Dettmers 26 Comments [...] Filed Under: Deep Learning, Hardware Tagged With: AMD, CPU, High Performance Computing, Matrix Multiplication, Parallel Computing, PCIe Lanes, Sparse Training\\\\n\\\\nLLM.int8() and Emergent Features\\\\n\\\\n2022-08-17 by Tim Dettmers 13 Comments [...] [Read more…] about How to Choose Your Grad School\\\\n\\\\nFiled Under: Academia, PhD Life Tagged With: Advisors, Grad school, PhD\\\\n\\\\nOn Creativity in Academia\\\\n\\\\n2019-09-03 by Tim Dettmers 5 Comments\"}, {\"url\": \"https://twitter.com/Tim_Dettmers\", \"content\": \"Perplexity Deep Research is quite close to OpenAI o3 on the Humanity Last Exam Benchmark despite being an order of magnitude faster and cheaper. This is\"}, {\"url\": \"https://developer.nvidia.com/blog/author/tdettmers/\", \"content\": \"He runs a blog about deep learning and takes part in Kaggle data science competitions where he has reached a world rank of 63. Follow @Tim_Dettmers on Twitter\"}, {\"url\": \"https://threadreaderapp.com/thread/1666076553665744896.html\", \"content\": \"Make a small donation by buying us coffee ($5) or help with server cost ($10)\\\\n\\\\nOr Donate anonymously using crypto!\\\\n\\\\nEthereum\\\\n\\\\n0xfe58350B80634f60Fa6Dc149a72b4DFbc17D341E\\\\ncopy\\\\n\\\\nBitcoin\\\\n\\\\n3ATGMxNzCUFzxpMCHL5sWSt4DVtS8UqXpi\\\\ncopy\\\\n\\\\nThank you for your support!\\\\n\\\\nSend Email!\\\\n\\\\nEmail the whole thread instead of just a link! [...] More from @Tim_Dettmers\\\\n\\\\nTim Dettmers\\\\n\\\\nTim Dettmers\\\\n\\\\nTim Dettmers\\\\n\\\\nTim Dettmers\\\\n\\\\nTim Dettmers\\\\n\\\\nTim Dettmers\\\\n\\\\nDid Thread Reader help you today?\\\\n\\\\nSupport us! We are indie developers!\\\\n\\\\nThis site is made by just two indie developers on a laptop doing marketing, support and development! Read more about the story.\\\\n\\\\nBecome a Premium Member ($3/month or $30/year) and get exclusive features!\\\\n\\\\nBecome Premium\\\\n\\\\nDon\\'t want to be a Premium member but still want to support us?\"}]', name='tavily_search_results_json', id='b68d8867-43ed-42c7-b189-e63381222187', tool_call_id='call_SnURBwhO1j5LRdld5226x3pb', artifact={'query': 'Tim Dettmers latest tweet', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://threadreaderapp.com/user/Tim_Dettmers', 'title': \"Tim Dettmers's Threads - Thread Reader App\", 'content': 'Oct 8, 2021 • 13 tweets • 5 min read. I am excited to share my latest work: 8-bit optimizers – a replacement for regular optimizers. Faster , 75% less', 'score': 0.73138535, 'raw_content': None}, {'url': 'https://timdettmers.com/', 'title': 'Tim Dettmers — Making deep learning accessible.', 'content': 'Filed Under: Academia, PhD Life Tagged With: Advisors, Grad school, PhD\\n\\nTPUs vs GPUs for Transformers (BERT)\\n\\n2018-10-17 by Tim Dettmers 26 Comments [...] Filed Under: Deep Learning, Hardware Tagged With: AMD, CPU, High Performance Computing, Matrix Multiplication, Parallel Computing, PCIe Lanes, Sparse Training\\n\\nLLM.int8() and Emergent Features\\n\\n2022-08-17 by Tim Dettmers 13 Comments [...] [Read more…] about How to Choose Your Grad School\\n\\nFiled Under: Academia, PhD Life Tagged With: Advisors, Grad school, PhD\\n\\nOn Creativity in Academia\\n\\n2019-09-03 by Tim Dettmers 5 Comments', 'score': 0.6649108, 'raw_content': None}, {'url': 'https://twitter.com/Tim_Dettmers', 'title': 'Tim Dettmers (@Tim_Dettmers) / X', 'content': 'Perplexity Deep Research is quite close to OpenAI o3 on the Humanity Last Exam Benchmark despite being an order of magnitude faster and cheaper. This is', 'score': 0.6147415, 'raw_content': None}, {'url': 'https://developer.nvidia.com/blog/author/tdettmers/', 'title': 'Author: Tim Dettmers | NVIDIA Technical Blog', 'content': 'He runs a blog about deep learning and takes part in Kaggle data science competitions where he has reached a world rank of 63. Follow @Tim_Dettmers on Twitter', 'score': 0.58948064, 'raw_content': None}, {'url': 'https://threadreaderapp.com/thread/1666076553665744896.html', 'title': 'Thread by @Tim_Dettmers on Thread Reader App', 'content': \"Make a small donation by buying us coffee ($5) or help with server cost ($10)\\n\\nOr Donate anonymously using crypto!\\n\\nEthereum\\n\\n0xfe58350B80634f60Fa6Dc149a72b4DFbc17D341E\\ncopy\\n\\nBitcoin\\n\\n3ATGMxNzCUFzxpMCHL5sWSt4DVtS8UqXpi\\ncopy\\n\\nThank you for your support!\\n\\nSend Email!\\n\\nEmail the whole thread instead of just a link! [...] More from @Tim_Dettmers\\n\\nTim Dettmers\\n\\nTim Dettmers\\n\\nTim Dettmers\\n\\nTim Dettmers\\n\\nTim Dettmers\\n\\nTim Dettmers\\n\\nDid Thread Reader help you today?\\n\\nSupport us! We are indie developers!\\n\\nThis site is made by just two indie developers on a laptop doing marketing, support and development! Read more about the story.\\n\\nBecome a Premium Member ($3/month or $30/year) and get exclusive features!\\n\\nBecome Premium\\n\\nDon't want to be a Premium member but still want to support us?\", 'score': 0.5492786, 'raw_content': None}], 'response_time': 2.56}), ToolMessage(content='[{\"url\": \"https://artidoro.github.io/\", \"content\": \"I have recently developed the Byte Latent Transformer, a new architecture that efficiently learns from raw byte data unlocking a new scaling dimension and paving the path towards universal byte models. With QLoRA, I reduced finetuning memory requirements by 15x and showed how to approach ChatGPT 3.5 performance in 24h on a single GPU. [...] Previously, I have also worked on sythetic data augmentation for improved controllability of generation systems, investigated language models’ reasoning and world modeling abilities, and evaluated their factual errors, as well as societal challenge associated with their use. [...] Artidoro Pagnoni\\\\n\\\\nPhD student in NLP at the University of Washington\\\\n\\\\nCiao!\\\\n\\\\nI am a final-year PhD student in Computer Science at the University of Washington, advised by Luke Zettlemoyer, and a visiting researcher at Meta. My research focuses on resource efficiency and improving LLM scaling trends.\"}, {\"url\": \"https://twitter.com/ArtidoroPagnoni/highlights\", \"content\": \"Artidoro Pagnoni\\'s Highlights ... Introducing the Byte Latent Transformer (BLT) – An LLM architecture that scales better than Llama 3 using byte-patches instead\"}, {\"url\": \"https://x.com/artidoropagnoni?lang=en\", \"content\": \"https://huggingface.co/facebook/blt Hoping to see many new and improved BLT based architectures this year!\"}, {\"url\": \"https://twitter.com/apagnonipagnoni?lang=ms\", \"content\": \"Don\\'t let climate change deniers derail progress in the global effort to #ActOnClimate: http://ofa.bo/e5FW. Imej. 204. 565. 1K. Artidoro Pagnoni menyiarkan\"}, {\"url\": \"https://x.com/ArtidoroPagnoni/status/1867601413741981804\", \"content\": \"Artidoro Pagnoni · @ArtidoroPagnoni. Introducing the Byte Latent Transformer (BLT) – An LLM architecture that scales better than Llama 3\"}]', name='tavily_search_results_json', id='81c4f5b8-f33c-47b4-a6c3-5cb5a6c963a3', tool_call_id='call_SlKHQQNCyr0C76eRnYpVLWCS', artifact={'query': 'Artidoro Pagnoni latest tweet', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://artidoro.github.io/', 'title': 'Artidoro Pagnoni: Ciao!', 'content': 'I have recently developed the Byte Latent Transformer, a new architecture that efficiently learns from raw byte data unlocking a new scaling dimension and paving the path towards universal byte models. With QLoRA, I reduced finetuning memory requirements by 15x and showed how to approach ChatGPT 3.5 performance in 24h on a single GPU. [...] Previously, I have also worked on sythetic data augmentation for improved controllability of generation systems, investigated language models’ reasoning and world modeling abilities, and evaluated their factual errors, as well as societal challenge associated with their use. [...] Artidoro Pagnoni\\n\\nPhD student in NLP at the University of Washington\\n\\nCiao!\\n\\nI am a final-year PhD student in Computer Science at the University of Washington, advised by Luke Zettlemoyer, and a visiting researcher at Meta. My research focuses on resource efficiency and improving LLM scaling trends.', 'score': 0.64746445, 'raw_content': None}, {'url': 'https://twitter.com/ArtidoroPagnoni/highlights', 'title': 'Highlights by Artidoro Pagnoni (@ArtidoroPagnoni) / X', 'content': \"Artidoro Pagnoni's Highlights ... Introducing the Byte Latent Transformer (BLT) – An LLM architecture that scales better than Llama 3 using byte-patches instead\", 'score': 0.64719695, 'raw_content': None}, {'url': 'https://x.com/artidoropagnoni?lang=en', 'title': 'Artidoro Pagnoni (@ArtidoroPagnoni) / X', 'content': 'https://huggingface.co/facebook/blt Hoping to see many new and improved BLT based architectures this year!', 'score': 0.6347929, 'raw_content': None}, {'url': 'https://twitter.com/apagnonipagnoni?lang=ms', 'title': 'Artidoro Pagnoni (@ApagnoniPagnoni) / X', 'content': \"Don't let climate change deniers derail progress in the global effort to #ActOnClimate: http://ofa.bo/e5FW. Imej. 204. 565. 1K. Artidoro Pagnoni menyiarkan\", 'score': 0.61168414, 'raw_content': None}, {'url': 'https://x.com/ArtidoroPagnoni/status/1867601413741981804', 'title': 'Artidoro Pagnoni - X', 'content': 'Artidoro Pagnoni · @ArtidoroPagnoni. Introducing the Byte Latent Transformer (BLT) – An LLM architecture that scales better than Llama 3', 'score': 0.60274047, 'raw_content': None}], 'response_time': 3.31}), ToolMessage(content='[{\"url\": \"https://x.com/aaholtzman?lang=en\", \"content\": \"In other news, I\\'ll be joining @UChicagoCS and @DSI_UChicago in 2024 as an assistant professor and doing a postdoc @Meta in the meantime! I\\'m at\"}, {\"url\": \"https://twitter.com/universeinanegg/status/1848794897773650368\", \"content\": \"UChicago has a fast growing Communication & Intelligence community, that @ChenhaoTan and @MinaLee__ co-founded along with me.\"}, {\"url\": \"https://openreview.net/pdf/4acd113560a311d91da11c5d522c8646641888ba.pdf\", \"content\": \"660 Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and 661 Yejin Choi. 2020. The curious case of neural text de-662 generation. In International Conference on Learning 663 Representations.\\\\n664 9 Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan 665 Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and 666 Weizhu Chen. 2022. LoRA: Low-rank adaptation of 667 large language models. In International Conference 668 on Learning Representations. [...] 279 Concretely, we first prompt the LLM and employ 280 top-p sampling (Holtzman et al., 2020) to sample 281 varying responses (to form the readers’ view from 282 diverse angles). Then, we compute their semantic 283 similarity to measure the responses’ consistency to 284 the source message. Lastly, we rank the responses 285 based on similarity to select the most relevant ones 286 as the auto-response for the remaining learning 287 4 process. The prompt is shown in Appendix D. [...] 14.18 14.57 0.66 83.55 11.59 8.52 8.66 0.37 81.27 LLaMA2-Response 17.21 11.81 12.30 0.56 83.12 11.37 8.03 8.46 0.37 80.43 LLaMA2-FT 18.37 12.13 13.11 0.61 82.01 11.21 8.11 8.27 0.31 80.81 RePALM 23.26 15.98 16.33 0.74 84.71 14.18 10.69 11.98 0.51 83.32 -w/o Response Augmentation 20.79 14.78 15.03 0.63 83.12 12.01 9.11 9.34 0.33 82.07 -w/o Dual-Reward Modeling 21.37 14.34 16.21 0.72 83.78 14.01 10.12 11.67 0.53 81.79 -w/o Reward Sampling 22.65 15.67 16.51 0.72 84.59 13.93 10.61 11.77 0.43 81.84\"}, {\"url\": \"https://x.com/universeinanegg?lang=en\", \"content\": \"Ari Holtzman\\'s posts ... If you want a respite from OpenAI drama, how about joining academia? I\\'m starting Conceptualization Lab, recruiting PhDs & Postdocs! We\"}, {\"url\": \"https://aclanthology.org/2024.lrec-main.986.pdf\", \"content\": \"Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. In Advances in Neural Information Processing Systems, volume 32. Curran Asso-ciates, Inc.\\\\nXinyi Zhou and Reza Zafarani. 2020. A survey of fake news: Fundamental theories, detection methods, and opportunities. ACM Computing Surveys (CSUR), 53(5):1–40. [...] Query-2 arma 3 russia ukraine lang:en -is:retweet 2021-12-21 2022-04-06 100 Query-3 for Other russia ukraine war video lang:en (has:media OR has:geo) -is:retweet 2021-12-21 2022-04-06 50 Query-4 for Other russia ukraine war video lang:en -is:retweet 2021-12-21 2022-04-06 50 Table 6: An example event in the dataset. A part of URL is cropped due to space constraints. [...] 2022-04 2022-05 0.000 0.001 0.002 0.003 0.004 0.005 0.006 Density EN-Covid Event EN12 EN13 EN14 EN15 EN16 EN17 EN18 EN19 EN20 (b) COVID-19 in English 2020-09 2020-11 2021-01 2021-03 2021-05 2021-07 2021-09 2021-11 2022-01 2022-03 2022-05 0.00 0.02 0.04 0.06 0.08 0.10 Density EN-Immigration EN21 EN22 EN23 EN24 EN25 EN26 EN27 EN28 EN29 EN30 (c) Immigration in English 2022-02-01 2022-02-15 2022-03-01 2022-03-15 2022-04-01 2022-04-15 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 Density TR-Ukraine TR01\"}]', name='tavily_search_results_json', id='01715183-c8c9-440b-90e1-85f7f2cb5060', tool_call_id='call_2ByCmkQzV7Qo9ApQb58yUUeQ', artifact={'query': 'Ari Holtzman latest tweet', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://x.com/aaholtzman?lang=en', 'title': 'Ari Holtzman (@aaholtzman) / X', 'content': \"In other news, I'll be joining @UChicagoCS and @DSI_UChicago in 2024 as an assistant professor and doing a postdoc @Meta in the meantime! I'm at\", 'score': 0.64478505, 'raw_content': None}, {'url': 'https://twitter.com/universeinanegg/status/1848794897773650368', 'title': 'Ari Holtzman on X: \"UChicago has a fast growing Communication ...', 'content': 'UChicago has a fast growing Communication & Intelligence community, that @ChenhaoTan and @MinaLee__ co-founded along with me.', 'score': 0.5917474, 'raw_content': None}, {'url': 'https://openreview.net/pdf/4acd113560a311d91da11c5d522c8646641888ba.pdf', 'title': '[PDF] RePALM: Popular Quote Tweet Generation via Auto ... - OpenReview', 'content': '660 Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and 661 Yejin Choi. 2020. The curious case of neural text de-662 generation. In International Conference on Learning 663 Representations.\\n664 9 Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan 665 Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and 666 Weizhu Chen. 2022. LoRA: Low-rank adaptation of 667 large language models. In International Conference 668 on Learning Representations. [...] 279 Concretely, we first prompt the LLM and employ 280 top-p sampling (Holtzman et al., 2020) to sample 281 varying responses (to form the readers’ view from 282 diverse angles). Then, we compute their semantic 283 similarity to measure the responses’ consistency to 284 the source message. Lastly, we rank the responses 285 based on similarity to select the most relevant ones 286 as the auto-response for the remaining learning 287 4 process. The prompt is shown in Appendix D. [...] 14.18 14.57 0.66 83.55 11.59 8.52 8.66 0.37 81.27 LLaMA2-Response 17.21 11.81 12.30 0.56 83.12 11.37 8.03 8.46 0.37 80.43 LLaMA2-FT 18.37 12.13 13.11 0.61 82.01 11.21 8.11 8.27 0.31 80.81 RePALM 23.26 15.98 16.33 0.74 84.71 14.18 10.69 11.98 0.51 83.32 -w/o Response Augmentation 20.79 14.78 15.03 0.63 83.12 12.01 9.11 9.34 0.33 82.07 -w/o Dual-Reward Modeling 21.37 14.34 16.21 0.72 83.78 14.01 10.12 11.67 0.53 81.79 -w/o Reward Sampling 22.65 15.67 16.51 0.72 84.59 13.93 10.61 11.77 0.43 81.84', 'score': 0.55275756, 'raw_content': None}, {'url': 'https://x.com/universeinanegg?lang=en', 'title': 'Ari Holtzman (@universeinanegg) / X', 'content': \"Ari Holtzman's posts ... If you want a respite from OpenAI drama, how about joining academia? I'm starting Conceptualization Lab, recruiting PhDs & Postdocs! We\", 'score': 0.5402697, 'raw_content': None}, {'url': 'https://aclanthology.org/2024.lrec-main.986.pdf', 'title': '[PDF] An Annotated Multi-Event Tweet Dataset for Misinformation Detection', 'content': 'Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. In Advances in Neural Information Processing Systems, volume 32. Curran Asso-ciates, Inc.\\nXinyi Zhou and Reza Zafarani. 2020. A survey of fake news: Fundamental theories, detection methods, and opportunities. ACM Computing Surveys (CSUR), 53(5):1–40. [...] Query-2 arma 3 russia ukraine lang:en -is:retweet 2021-12-21 2022-04-06 100 Query-3 for Other russia ukraine war video lang:en (has:media OR has:geo) -is:retweet 2021-12-21 2022-04-06 50 Query-4 for Other russia ukraine war video lang:en -is:retweet 2021-12-21 2022-04-06 50 Table 6: An example event in the dataset. A part of URL is cropped due to space constraints. [...] 2022-04 2022-05 0.000 0.001 0.002 0.003 0.004 0.005 0.006 Density EN-Covid Event EN12 EN13 EN14 EN15 EN16 EN17 EN18 EN19 EN20 (b) COVID-19 in English 2020-09 2020-11 2021-01 2021-03 2021-05 2021-07 2021-09 2021-11 2022-01 2022-03 2022-05 0.00 0.02 0.04 0.06 0.08 0.10 Density EN-Immigration EN21 EN22 EN23 EN24 EN25 EN26 EN27 EN28 EN29 EN30 (c) Immigration in English 2022-02-01 2022-02-15 2022-03-01 2022-03-15 2022-04-01 2022-04-15 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 Density TR-Ukraine TR01', 'score': 0.52597815, 'raw_content': None}], 'response_time': 2.28}), ToolMessage(content='[{\"url\": \"https://twitter.com/LukeZettlemoyer\", \"content\": \"Our latest work uses multi-agent reinforcement learning to model corporate investment in climate change mitigation as a social dilemma. We create a new\"}, {\"url\": \"https://twitter.com/lukezettlemoyer/status/1003662931479941120\", \"content\": \"Come see Julian Michael presenting his work in question answer meaning representations, now at @NAACLHLT ! Image. 3:41 PM · Jun 4, 2018.\"}, {\"url\": \"https://x.com/lukezettlemoyer?lang=en\", \"content\": \"New paper from FAIR+NYU: Q: Is language supervision required to learn effective visual representations for multimodal tasks? A: No.\"}, {\"url\": \"https://openreview.net/pdf/1cb1687474929f19b009c09c143eca5e9724e2af.pdf\", \"content\": \"691 In Proceedings of the 2018 Conference on Empiri-692 cal Methods in Natural Language Processing, pages 693 659–664, Brussels, Belgium. Association for Com-694 putational Linguistics.\\\\n695 Mike Lewis, Yinhan Liu, Naman Goyal, Marjan 696 Ghazvininejad, Abdelrahman Mohamed, Omer Levy, 697 Veselin Stoyanov, and Luke Zettlemoyer. 2020. [...] Length 26.08 30.70 44.24 49.52 Verb 3.80 4.48 5.95 8.31 Noun 8.05 9.57 13.90 16.13 Adjective 1.82 2.14 3.23 4.15 Hashtag 1.04 1.16 3.10 1.76 Positive 1.42 1.63 2.90 3.68 Negative 1.06 1.33 1.91 1.66 Generality.\\\\nIndef 0.54 0.67 0.89 1.30 Def 1.13 1.27 1.78 2.27 Readability.\\\\nFlesch Score ↑ 44.71 48.75 23.07 24.71 Flesch Level ↓ 13.79 12.12 18.75 14.84 Imitation. [...] LLaMA2-Chat: Pondering the future of #customersupport: Will #ChatGPT be the silver bullet for orgs? #AI #language-model ChatGPT: Revolutionizing Customer Support with Chat-GPT! Discover how language models like ChatGPT are not just conversing and blogging, but also acing legal tests. Is this the future of customer service? #ChatGPT #CustomerServi-ceInnovation #AIRevolution RaPALM: Just set up my ChatGPT and I’m blown away by its capabilities! Just learn and try it on your customer support team.\"}, {\"url\": \"https://news.cs.washington.edu/2017/10/22/luke-zettlemoyer-allen-institute-for-ai-in-ny-times/\", \"content\": \"Allen School News\\\\n\\\\nLuke Zettlemoyer, Allen Institute for AI in NY Times\\\\n\\\\n“Luke Zettlemoyer, a professor at the University of Washington … turned down a lucrative offer from Google, instead taking a post at the nonprofit Allen Institute for Artificial Intelligence so he could continue teaching.”\\\\n\\\\nLuke and Ali Farhadi are heavily engaged in AI2, which is led by long-time Allen School professor Oren Etzioni. It offers the best of both worlds.\\\\n\\\\nNY Times article here.\\\\n\\\\nConnect With #UWAllen [...] Undergraduate Advising: ugrad-adviser at cs.washington.edu\\\\n                            Professional Master\\'s Program Advising: masters at cs.washington.edu\\\\n                            Graduate (Ph.D.) Advising: grad-advising at cs.washington.edu\\\\n\\\\nCopyright © 2017-2021 University of Washington - Seattle -\\\\n                        Paul G. Allen School of Computer Science & Engineering | All Rights |\\\\n                        Privacy |\\\\n                        Terms [...] Contact us:\\\\n                            Main Tel: (206) 543-1695\\\\n                            Media Inquiries: media at cs.washington.edu\\\\n                            Webmaster: support at cs.washington.edu\\\\n\\\\nComputer Engineering degree program accredited by ABET\"}]', name='tavily_search_results_json', id='f54e1422-a3cc-4f91-86a3-e2e6873a27cf', tool_call_id='call_v1fOigWeGA30JwuiI1BpEzKP', artifact={'query': 'Luke Zettlemoyer latest tweet', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://twitter.com/LukeZettlemoyer', 'title': 'Luke Zettlemoyer (@LukeZettlemoyer) / X', 'content': 'Our latest work uses multi-agent reinforcement learning to model corporate investment in climate change mitigation as a social dilemma. We create a new', 'score': 0.6721816, 'raw_content': None}, {'url': 'https://twitter.com/lukezettlemoyer/status/1003662931479941120', 'title': 'Luke Zettlemoyer on X: \"Come see Julian Michael presenting his ...', 'content': 'Come see Julian Michael presenting his work in question answer meaning representations, now at @NAACLHLT ! Image. 3:41 PM · Jun 4, 2018.', 'score': 0.6147415, 'raw_content': None}, {'url': 'https://x.com/lukezettlemoyer?lang=en', 'title': 'Luke Zettlemoyer (@LukeZettlemoyer) / X', 'content': 'New paper from FAIR+NYU: Q: Is language supervision required to learn effective visual representations for multimodal tasks? A: No.', 'score': 0.5911811, 'raw_content': None}, {'url': 'https://openreview.net/pdf/1cb1687474929f19b009c09c143eca5e9724e2af.pdf', 'title': '[PDF] Popular Quoting Tweet Generation via Auto ... - OpenReview', 'content': '691 In Proceedings of the 2018 Conference on Empiri-692 cal Methods in Natural Language Processing, pages 693 659–664, Brussels, Belgium. Association for Com-694 putational Linguistics.\\n695 Mike Lewis, Yinhan Liu, Naman Goyal, Marjan 696 Ghazvininejad, Abdelrahman Mohamed, Omer Levy, 697 Veselin Stoyanov, and Luke Zettlemoyer. 2020. [...] Length 26.08 30.70 44.24 49.52 Verb 3.80 4.48 5.95 8.31 Noun 8.05 9.57 13.90 16.13 Adjective 1.82 2.14 3.23 4.15 Hashtag 1.04 1.16 3.10 1.76 Positive 1.42 1.63 2.90 3.68 Negative 1.06 1.33 1.91 1.66 Generality.\\nIndef 0.54 0.67 0.89 1.30 Def 1.13 1.27 1.78 2.27 Readability.\\nFlesch Score ↑ 44.71 48.75 23.07 24.71 Flesch Level ↓ 13.79 12.12 18.75 14.84 Imitation. [...] LLaMA2-Chat: Pondering the future of #customersupport: Will #ChatGPT be the silver bullet for orgs? #AI #language-model ChatGPT: Revolutionizing Customer Support with Chat-GPT! Discover how language models like ChatGPT are not just conversing and blogging, but also acing legal tests. Is this the future of customer service? #ChatGPT #CustomerServi-ceInnovation #AIRevolution RaPALM: Just set up my ChatGPT and I’m blown away by its capabilities! Just learn and try it on your customer support team.', 'score': 0.48982134, 'raw_content': None}, {'url': 'https://news.cs.washington.edu/2017/10/22/luke-zettlemoyer-allen-institute-for-ai-in-ny-times/', 'title': 'Luke Zettlemoyer, Allen Institute for AI in NY Times', 'content': \"Allen School News\\n\\nLuke Zettlemoyer, Allen Institute for AI in NY Times\\n\\n“Luke Zettlemoyer, a professor at the University of Washington … turned down a lucrative offer from Google, instead taking a post at the nonprofit Allen Institute for Artificial Intelligence so he could continue teaching.”\\n\\nLuke and Ali Farhadi are heavily engaged in AI2, which is led by long-time Allen School professor Oren Etzioni. It offers the best of both worlds.\\n\\nNY Times article here.\\n\\nConnect With #UWAllen [...] Undergraduate Advising: ugrad-adviser at cs.washington.edu\\n                            Professional Master's Program Advising: masters at cs.washington.edu\\n                            Graduate (Ph.D.) Advising: grad-advising at cs.washington.edu\\n\\nCopyright © 2017-2021 University of Washington - Seattle -\\n                        Paul G. Allen School of Computer Science & Engineering | All Rights |\\n                        Privacy |\\n                        Terms [...] Contact us:\\n                            Main Tel: (206) 543-1695\\n                            Media Inquiries: media at cs.washington.edu\\n                            Webmaster: support at cs.washington.edu\\n\\nComputer Engineering degree program accredited by ABET\", 'score': 0.35307175, 'raw_content': None}], 'response_time': 3.17})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='Here are the latest tweets or related information for the authors of the QLoRA paper:\\n\\n1. **Tim Dettmers**:\\n   - Latest Tweet: Tim Dettmers has shared insights on \"Perplexity Deep Research\" being close to OpenAI o3 on the Humanity Last Exam Benchmark, highlighting its speed and cost efficiency. [Source](https://twitter.com/Tim_Dettmers)\\n\\n2. **Artidoro Pagnoni**:\\n   - Latest Tweet: Artidoro Pagnoni introduced the Byte Latent Transformer (BLT), an LLM architecture that scales better than Llama 3 using byte-patches. [Source](https://x.com/ArtidoroPagnoni/status/1867601413741981804)\\n\\n3. **Ari Holtzman**:\\n   - Latest Tweet: Ari Holtzman announced joining the University of Chicago as an assistant professor in 2024 and doing a postdoc at Meta in the meantime. [Source](https://x.com/aaholtzman?lang=en)\\n\\n4. **Luke Zettlemoyer**:\\n   - Latest Tweet: Luke Zettlemoyer discussed a new paper from FAIR+NYU questioning the necessity of language supervision for learning effective visual representations for multimodal tasks. [Source](https://x.com/lukezettlemoyer?lang=en)\\n\\nRegarding Wikipedia pages, Ari Holtzman has a Wikipedia page, but there are no specific pages for Tim Dettmers, Artidoro Pagnoni, or Luke Zettlemoyer.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 317, 'prompt_tokens': 6610, 'total_tokens': 6927, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 3200}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a6889ffe71', 'finish_reason': 'stop', 'logprobs': None}, id='run-93025c69-4d96-42ba-b7c0-0da71b009c80-0', usage_metadata={'input_tokens': 6610, 'output_tokens': 317, 'total_tokens': 6927, 'input_token_details': {'audio': 0, 'cache_read': 3200}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Restricted to a steps_taken of 5 or fewer.... The LLM completes the task and provides the final result.\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the QLoRA paper, identify the authors and find if they have any Wikipedia pages, then search each of the authors to find out their latest Tweet using Tavily!\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        if node == \"action\":\n",
        "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
        "        print(values[\"messages\"])\n",
        "\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "#### 🏗️ Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer.\n",
        "\n",
        "1. Step 1: Agent called model.\n",
        "2. Step 1: Tool executed. - Arxiv\n",
        "3. Step 2: Agent called model.\n",
        "4. Step 2: Tool executed. - Wikipedia\n",
        "5. Step 3: Agent called model.\n",
        "6. Step 3: Tool executed. - Tavily\n",
        "7. Step 4: Agent called model.\n",
        "8. Step 4: No tool call. Ending.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🤝 Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "## Part 1: LangSmith Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | compiled_graph | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "76be837b-6424-4516-8f63-07fbd8c25bf5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/viveknatan/Documents/AIMCourse/agent/05_Our_First_Agent_with_LangGraph/.venv/lib/python3.13/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /Users/viveknatan/Documents/AIMCourse/agent/05_Our_First_Agent_with_LangGraph/.venv/lib/python3.13/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'In the context of AI, RAG stands for \"Retrieval-Augmented Generation.\" It is a technique that enhances generative AI models by allowing them to retrieve and incorporate new information from external sources. This approach modifies interactions with a large language model (LLM) so that the model can respond to user queries by referencing a specified set of documents, supplementing the information from its pre-existing training data. This enables the use of domain-specific and updated information, improving the accuracy and relevance of the responses.\\n\\nRAG works by dynamically retrieving relevant text from databases, uploaded documents, or web sources before generating responses. This method helps reduce AI hallucinations and allows LLMs to provide more accurate responses without frequent retraining. Additionally, RAG can include source references in responses, enabling users to verify information by reviewing cited documents or original sources, thus providing greater transparency. The concept of RAG was introduced in 2020 by a group of researchers in their paper on knowledge-intensive NLP tasks.'"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG in the context of AI?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "#### 🏗️ Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ℹ️ Using existing dataset: qa_dataset\n",
            "✅ [1] Added: \"What optimizer is used in QLoRA?\"\n",
            "✅ [2] Added: \"What data type was created in the QLoRA paper?\"\n",
            "✅ [3] Added: \"What is a Retrieval Augmented Generation system?\"\n",
            "✅ [4] Added: \"Who authored the QLoRA paper?\"\n",
            "✅ [5] Added: \"What is the most popular deep learning framework?\"\n",
            "✅ [6] Added: \"What significant improvements does the LoRA system make?\"\n"
          ]
        }
      ],
      "source": [
        "from langsmith import Client\n",
        "client = Client()\n",
        "\n",
        "# dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "# dataset = client.create_dataset(\n",
        "#     dataset_name=dataset_name,\n",
        "#     description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        "# )\n",
        "\n",
        "# client.create_examples(\n",
        "#     inputs=[{\"question\" : q} for q in questions],\n",
        "#     outputs=answers,\n",
        "#     dataset_id=dataset.id,\n",
        "# )\n",
        "\n",
        "# === Validate Data ===\n",
        "if len(questions) != len(answers):\n",
        "    raise ValueError(f\"❌ Mismatch: {len(questions)} questions vs {len(answers)} answers.\")\n",
        "\n",
        "# === Dataset Creation ===\n",
        "dataset_name = \"qa_dataset\"\n",
        "try:\n",
        "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
        "    print(f\"ℹ️ Using existing dataset: {dataset.name}\")\n",
        "except Exception:\n",
        "    dataset = client.create_dataset(\n",
        "        dataset_name=dataset_name,\n",
        "        description=\"Basic QA examples for LangChain eval tests\"\n",
        "    )\n",
        "    print(f\"✅ Created new dataset: {dataset.name}\")\n",
        "\n",
        "# === Create Examples ===\n",
        "for idx, (question, expected) in enumerate(zip(questions, answers), 1):\n",
        "    try:\n",
        "        client.create_example(\n",
        "            inputs={\"question\": question},\n",
        "            outputs=expected,\n",
        "            dataset_id=dataset.id\n",
        "        )\n",
        "        print(f\"✅ [{idx}] Added: \\\"{question}\\\"\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ [{idx}] Failed: \\\"{question}\\\"\\n   ↳ Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'example_ids': ['f1833e2e-9e12-4455-b1c0-fba74f5d6f08',\n",
              "  '18b0170c-b9cb-4eb3-acef-43a886f4a6c1',\n",
              "  '49198dae-294a-4514-bf13-f03353360d52',\n",
              "  'eac5a16d-4843-431f-af0d-8239eb8abc12',\n",
              "  'add646cb-bb6d-4011-bb92-7871ec171bbc',\n",
              "  '1d77238c-b65a-4f77-b31e-04ae27d96db6'],\n",
              " 'count': 6}"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ❓ Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not\n",
        "\n",
        "A: The code implicitly assumes that the order of questions and answers matches 1-to-1. It also assumes that there will be the same number of questions and answers. I have included some changes to ensure tht the same number of questions and answers exist, ensure explicit pairing of questions and answers and exception handling and logging for failures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here is a slightly evolved version which corrects for case sensitivity and also credits partial scores for must contain.\n",
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention_v2(run, example) -> EvaluationResult:\n",
        "    prediction = (run.outputs.get(\"output\") or \"\").lower()\n",
        "    required = [r.lower() for r in example.outputs.get(\"must_mention\", [])]\n",
        "\n",
        "    matches = [phrase for phrase in required if phrase in prediction]\n",
        "    score = len(matches) / len(required) if required else 1.0\n",
        "\n",
        "    return EvaluationResult(\n",
        "        key=\"must_mention\",\n",
        "        score=score,\n",
        "        comment=f\"Matched: {matches} | Missing: {[r for r in required if r not in matches]}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ❓ Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
        "\n",
        "The must_mention evaluator is deterministic, fast, easy to implement and usefulf for use cases with hard requirements. \n",
        "It does have some gaps, notably\n",
        "1. It looks at the Exact string match\n",
        "2. Doesnt account for case sensitivity\n",
        "3. Does not take partial credit into account\n",
        "4. It is not aware of the context and is just looking for the occurance of specific words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "efcf57067cf743d8b4ce059a61cbe02e",
            "53e33aae3b97490c82aec7bbb0d6ebba",
            "ad84e0e971d3455db2efe7dd0d1f803e",
            "72adef9b70dd48198b7322b6c5b113cf",
            "8a61d045ffd44ac58f3f13eb10044836",
            "041e22a9b5514e36bd4d1dac01d5d398",
            "886d762f2a7c421382efb5502c6d42a1",
            "ab91fd625bbd43afbf8c6398193a88d0",
            "716557ad09874dcb989d75f7c74424cd",
            "77d4c0ebaae045b58efc4f789c9a2360",
            "0d622ccc56264fac8fd7508dbdbe6e29"
          ]
        },
        "id": "p5TeCUUkuGld",
        "outputId": "2f7d62a2-e78d-447a-d07b-f9e4d500fb79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'RAG Pipeline - Evaluation - 55f8-343546e5' at:\n",
            "https://smith.langchain.com/o/c9d5843a-35e9-4d41-83e6-32315ffaf39f/datasets/0d265a91-3a56-4dbe-a59c-fb829be3dc1b/compare?selectedSessions=4bb6a869-ba79-4c18-b012-e5fc8a35dc03\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9095b45e9815447ea3bb8974e1fc11a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "experiment_results = client.evaluate(\n",
        "    agent_chain,\n",
        "    data=dataset_name,\n",
        "    evaluators=[must_mention],\n",
        "    experiment_prefix=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:4]}\",\n",
        "    metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "eeEqU7s05Byu",
        "outputId": "78395075-a05d-4ebd-c798-ed968b935318"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs.question</th>\n",
              "      <th>outputs.output</th>\n",
              "      <th>error</th>\n",
              "      <th>reference.must_mention</th>\n",
              "      <th>feedback.must_mention</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>example_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What significant improvements does the LoRA sy...</td>\n",
              "      <td>The LoRa system, which stands for \"Long Range,...</td>\n",
              "      <td>None</td>\n",
              "      <td>[reduce, parameters]</td>\n",
              "      <td>False</td>\n",
              "      <td>9.587728</td>\n",
              "      <td>04eab0ad-5fc4-4057-90a7-8676c698d8b6</td>\n",
              "      <td>e8188ad9-7648-40cf-91cd-45030ecf987e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the most popular deep learning framework?</td>\n",
              "      <td>In 2023, the most popular deep learning framew...</td>\n",
              "      <td>None</td>\n",
              "      <td>[PyTorch, TensorFlow]</td>\n",
              "      <td>True</td>\n",
              "      <td>6.462374</td>\n",
              "      <td>3f900e87-3148-4956-bcc7-b1a8b4b1f62c</td>\n",
              "      <td>31b9983c-1bff-4d35-b534-6c8e00d74ac9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Who authored the QLoRA paper?</td>\n",
              "      <td>The QLoRA paper titled \"Accurate LoRA-Finetuni...</td>\n",
              "      <td>None</td>\n",
              "      <td>[Tim, Dettmers]</td>\n",
              "      <td>False</td>\n",
              "      <td>2.734182</td>\n",
              "      <td>f2b843fe-c0ae-4cfb-a332-61d728ea87b6</td>\n",
              "      <td>6f2efbe4-c6f0-4369-beb9-53dba57809d1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is a Retrieval Augmented Generation system?</td>\n",
              "      <td>Retrieval-augmented generation (RAG) is a tech...</td>\n",
              "      <td>None</td>\n",
              "      <td>[ground, context]</td>\n",
              "      <td>False</td>\n",
              "      <td>8.014176</td>\n",
              "      <td>9f9f6847-0f55-4e42-97a9-0c47f8ae923b</td>\n",
              "      <td>37cd6e57-714f-4b6a-b39b-1e5c3e246f2c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What data type was created in the QLoRA paper?</td>\n",
              "      <td>In the QLoRA paper, a new data type called \"4-...</td>\n",
              "      <td>None</td>\n",
              "      <td>[NF4, NormalFloat]</td>\n",
              "      <td>True</td>\n",
              "      <td>2.953289</td>\n",
              "      <td>7594421a-a611-4bb7-b4e5-4ee0e61e7832</td>\n",
              "      <td>6b4808ea-50c1-4a02-a319-b277598488f0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What optimizer is used in QLoRA?</td>\n",
              "      <td>QLoRA uses \"paged optimizers\" to manage memory...</td>\n",
              "      <td>None</td>\n",
              "      <td>[paged, optimizer]</td>\n",
              "      <td>True</td>\n",
              "      <td>2.453867</td>\n",
              "      <td>e8ae5a0b-504e-47cb-b62b-2ac3afea1a5d</td>\n",
              "      <td>9d1ef350-0a90-4880-a9a7-354606fd0a67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What significant improvements does the LoRA sy...</td>\n",
              "      <td>The LoRa system, which stands for \"Long Range,...</td>\n",
              "      <td>None</td>\n",
              "      <td>[reduce, parameters]</td>\n",
              "      <td>False</td>\n",
              "      <td>9.829428</td>\n",
              "      <td>7a608cab-d1c5-4cde-bbe7-fc9f9bd0d59b</td>\n",
              "      <td>3876092d-59a3-47c4-a2e1-24c5c66dce3b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What is the most popular deep learning framework?</td>\n",
              "      <td>In 2023, the most popular deep learning framew...</td>\n",
              "      <td>None</td>\n",
              "      <td>[PyTorch, TensorFlow]</td>\n",
              "      <td>True</td>\n",
              "      <td>3.781330</td>\n",
              "      <td>ae89f498-9b6f-4a10-87bc-97e4472cca99</td>\n",
              "      <td>44b6ca4d-45da-41e5-a0aa-ef02e59f5dcb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Who authored the QLoRA paper?</td>\n",
              "      <td>The QLoRA paper titled \"Accurate LoRA-Finetuni...</td>\n",
              "      <td>None</td>\n",
              "      <td>[Tim, Dettmers]</td>\n",
              "      <td>False</td>\n",
              "      <td>2.203096</td>\n",
              "      <td>45c765db-a98c-41e5-9136-cb4a854705da</td>\n",
              "      <td>c4311b54-7ce0-43ce-8f07-f238356734cd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What is a Retrieval Augmented Generation system?</td>\n",
              "      <td>Retrieval-augmented generation (RAG) is a tech...</td>\n",
              "      <td>None</td>\n",
              "      <td>[ground, context]</td>\n",
              "      <td>False</td>\n",
              "      <td>6.599669</td>\n",
              "      <td>6f31743d-ef5f-4205-b7af-6d2a628c783c</td>\n",
              "      <td>de55f412-8631-4f85-a449-71c4ae8ba18e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>What data type was created in the QLoRA paper?</td>\n",
              "      <td>In the QLoRA paper, a new data type called \"4-...</td>\n",
              "      <td>None</td>\n",
              "      <td>[NF4, NormalFloat]</td>\n",
              "      <td>True</td>\n",
              "      <td>2.916539</td>\n",
              "      <td>c6a3719c-b983-4bc2-bfbd-7171feb75de7</td>\n",
              "      <td>70dbe1a3-4063-4e27-93e8-e0d36707fca8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What optimizer is used in QLoRA?</td>\n",
              "      <td>QLoRA uses \"paged optimizers\" to manage memory...</td>\n",
              "      <td>None</td>\n",
              "      <td>[paged, optimizer]</td>\n",
              "      <td>True</td>\n",
              "      <td>3.104299</td>\n",
              "      <td>0c48b394-04a7-44ad-9a27-9d7967292d08</td>\n",
              "      <td>4c17ff04-4843-4c53-a90c-ccc6e66c518b</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "<ExperimentResults RAG Pipeline - Evaluation - 55f8-343546e5>"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "experiment_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'RAG Pipeline - Evaluation - 7267-7fca39f2' at:\n",
            "https://smith.langchain.com/o/c9d5843a-35e9-4d41-83e6-32315ffaf39f/datasets/0d265a91-3a56-4dbe-a59c-fb829be3dc1b/compare?selectedSessions=1273f758-0865-48b6-af6f-77be990d7c7c\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94aff5787b6d4c3692fcc35d2a1eed7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "experiment_results_v2 = client.evaluate(\n",
        "    agent_chain,\n",
        "    data=dataset_name,\n",
        "    evaluators=[must_mention_v2],\n",
        "    experiment_prefix=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:4]}\",\n",
        "    metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs.question</th>\n",
              "      <th>outputs.output</th>\n",
              "      <th>error</th>\n",
              "      <th>reference.must_mention</th>\n",
              "      <th>feedback.must_mention</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>example_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What significant improvements does the LoRA sy...</td>\n",
              "      <td>The LoRa system, which stands for \"Long Range,...</td>\n",
              "      <td>None</td>\n",
              "      <td>[reduce, parameters]</td>\n",
              "      <td>0.5</td>\n",
              "      <td>10.237442</td>\n",
              "      <td>04eab0ad-5fc4-4057-90a7-8676c698d8b6</td>\n",
              "      <td>095a703c-e1ef-43c6-b961-2fc0f92c029d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the most popular deep learning framework?</td>\n",
              "      <td>In 2023, the most popular deep learning framew...</td>\n",
              "      <td>None</td>\n",
              "      <td>[PyTorch, TensorFlow]</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.444331</td>\n",
              "      <td>3f900e87-3148-4956-bcc7-b1a8b4b1f62c</td>\n",
              "      <td>4fa5a265-097c-4afd-aeaf-fe83edf5c9a9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Who authored the QLoRA paper?</td>\n",
              "      <td>The QLoRA paper titled \"Accurate LoRA-Finetuni...</td>\n",
              "      <td>None</td>\n",
              "      <td>[Tim, Dettmers]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.525303</td>\n",
              "      <td>f2b843fe-c0ae-4cfb-a332-61d728ea87b6</td>\n",
              "      <td>e710625a-810d-4b48-8fbd-041928796313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is a Retrieval Augmented Generation system?</td>\n",
              "      <td>Retrieval-augmented generation (RAG) is a tech...</td>\n",
              "      <td>None</td>\n",
              "      <td>[ground, context]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.453169</td>\n",
              "      <td>9f9f6847-0f55-4e42-97a9-0c47f8ae923b</td>\n",
              "      <td>7e61fcf7-75cf-40f0-8ef9-ed71dc951f88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What data type was created in the QLoRA paper?</td>\n",
              "      <td>In the QLoRA paper, a new data type called \"4-...</td>\n",
              "      <td>None</td>\n",
              "      <td>[NF4, NormalFloat]</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.869476</td>\n",
              "      <td>7594421a-a611-4bb7-b4e5-4ee0e61e7832</td>\n",
              "      <td>46ea7626-e161-46c2-ae66-39898ba3cc42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What optimizer is used in QLoRA?</td>\n",
              "      <td>QLoRA uses \"paged optimizers\" to manage memory...</td>\n",
              "      <td>None</td>\n",
              "      <td>[paged, optimizer]</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.554624</td>\n",
              "      <td>e8ae5a0b-504e-47cb-b62b-2ac3afea1a5d</td>\n",
              "      <td>3f3abb29-19db-4be8-bdbc-cb60e6a08594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What significant improvements does the LoRA sy...</td>\n",
              "      <td>The LoRa system, which stands for \"Long Range,...</td>\n",
              "      <td>None</td>\n",
              "      <td>[reduce, parameters]</td>\n",
              "      <td>0.5</td>\n",
              "      <td>8.088757</td>\n",
              "      <td>7a608cab-d1c5-4cde-bbe7-fc9f9bd0d59b</td>\n",
              "      <td>5b57c739-fd23-4320-b499-54a0694a69b5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What is the most popular deep learning framework?</td>\n",
              "      <td>In 2023, the most popular deep learning framew...</td>\n",
              "      <td>None</td>\n",
              "      <td>[PyTorch, TensorFlow]</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.401117</td>\n",
              "      <td>ae89f498-9b6f-4a10-87bc-97e4472cca99</td>\n",
              "      <td>3dc26344-f932-4335-ab99-f979951a7106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Who authored the QLoRA paper?</td>\n",
              "      <td>The QLoRA paper titled \"Accurate LoRA-Finetuni...</td>\n",
              "      <td>None</td>\n",
              "      <td>[Tim, Dettmers]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.458254</td>\n",
              "      <td>45c765db-a98c-41e5-9136-cb4a854705da</td>\n",
              "      <td>5222dee8-a2d1-4c7b-a898-88208ded4a71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What is a Retrieval Augmented Generation system?</td>\n",
              "      <td>Retrieval-augmented generation (RAG) is a tech...</td>\n",
              "      <td>None</td>\n",
              "      <td>[ground, context]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.838947</td>\n",
              "      <td>6f31743d-ef5f-4205-b7af-6d2a628c783c</td>\n",
              "      <td>93195534-6984-481e-8d3d-734921a11c85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>What data type was created in the QLoRA paper?</td>\n",
              "      <td>The QLoRA paper introduced the concept of \"Qua...</td>\n",
              "      <td>None</td>\n",
              "      <td>[NF4, NormalFloat]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.563620</td>\n",
              "      <td>c6a3719c-b983-4bc2-bfbd-7171feb75de7</td>\n",
              "      <td>2ca6e82d-5575-4f79-b323-219914c53913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What optimizer is used in QLoRA?</td>\n",
              "      <td>QLoRA uses \"paged optimizers\" to manage memory...</td>\n",
              "      <td>None</td>\n",
              "      <td>[paged, optimizer]</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.463835</td>\n",
              "      <td>0c48b394-04a7-44ad-9a27-9d7967292d08</td>\n",
              "      <td>04bcf028-ad57-47cc-ac25-bbbd1a737981</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "<ExperimentResults RAG Pipeline - Evaluation - 7267-7fca39f2>"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "experiment_results_v2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add to our existing conditional edge to obtain the behaviour we desire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "#### 🏗️ Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r6XXA5FJbVf",
        "outputId": "ff713041-e498-4f0f-a875-a03502b87729"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x12e49f6b0>"
            ]
          },
          "execution_count": 164,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNWHwWxuRiLY",
        "outputId": "295f5a35-ceff-452a-ffb8-c52eada6a816"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x12e49f6b0>"
            ]
          },
          "execution_count": 165,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsXeF6xlaXOZ"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "z_Sq3A9SaV1O"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def tool_call_or_helpful(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    return \"end\"\n",
        "  else:\n",
        "    return \"continue\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "#### 🏗️ Activity #4:\n",
        "\n",
        "Please write what is happening in our `tool_call_or_helpful` function!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhnBW2YVsJO"
      },
      "source": [
        "The tool_call_or_helpful() function inspects the last message in the conversation state to decide what the agent does next. If the message includes a tool call, it returns \"action\"; if the answer is deemed helpful or a stopping condition is met, it returns \"end\". Otherwise, it returns \"continue\" to let the agent keep reasoning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVTKnWMbP_8T",
        "outputId": "7f729b1f-311c-4084-ceaf-0da437900c85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x12e49f6b0>"
            ]
          },
          "execution_count": 167,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tool_call_or_helpful,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"action\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "After a tool is called and executed, the result is returned to the agent to let it decide what to do next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbDK2MbuREgU",
        "outputId": "21a64c20-27a1-4e0e-afde-a639abaa8b55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x12e49f6b0>"
            ]
          },
          "execution_count": 168,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "Compile the state graph defined above into an executable agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "The code below executes the created agent in an async fashion to answer a multi-part question. It sends an initial HumanMessage as input, then asynchronously prints updates from each node (agent, action, etc.) as the agent processes the query. The printed output includes messages generated or handled by each node during the reasoning and tool-use loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "f152dea8-96ad-4d29-d8b2-a064c96a8bd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_FCvmv7FpjgSqjH1FvYo6PJhT', 'function': {'arguments': '{\"query\": \"LoRA machine learning\"}', 'name': 'wikipedia'}, 'type': 'function'}, {'id': 'call_gw2qwgPWEzS58oUk5AiRkY45', 'function': {'arguments': '{\"query\": \"Tim Dettmers\"}', 'name': 'wikipedia'}, 'type': 'function'}, {'id': 'call_Ql3pK64VSbXYSEj1nDHmEvfI', 'function': {'arguments': '{\"query\": \"Attention (machine learning)\"}', 'name': 'wikipedia'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 235, 'total_tokens': 301, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a6889ffe71', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-88576ea3-21c6-4722-af4f-7610d1ee4c22-0', tool_calls=[{'name': 'wikipedia', 'args': {'query': 'LoRA machine learning'}, 'id': 'call_FCvmv7FpjgSqjH1FvYo6PJhT', 'type': 'tool_call'}, {'name': 'wikipedia', 'args': {'query': 'Tim Dettmers'}, 'id': 'call_gw2qwgPWEzS58oUk5AiRkY45', 'type': 'tool_call'}, {'name': 'wikipedia', 'args': {'query': 'Attention (machine learning)'}, 'id': 'call_Ql3pK64VSbXYSEj1nDHmEvfI', 'type': 'tool_call'}], usage_metadata={'input_tokens': 235, 'output_tokens': 66, 'total_tokens': 301, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/viveknatan/Documents/AIMCourse/agent/05_Our_First_Agent_with_LangGraph/.venv/lib/python3.13/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /Users/viveknatan/Documents/AIMCourse/agent/05_Our_First_Agent_with_LangGraph/.venv/lib/python3.13/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='Page: Fine-tuning (deep learning)\\nSummary: In deep learning, fine-tuning is an approach to transfer learning in which the parameters of a pre-trained neural network model are trained on new data. Fine-tuning can be done on the entire neural network, or on only a subset of its layers, in which case the layers that are not being fine-tuned are \"frozen\" (i.e., not changed during backpropagation). A model may also be augmented with \"adapters\" that consist of far fewer parameters than the original model, and fine-tuned in a parameter-efficient way by tuning the weights of the adapters and leaving the rest of the model\\'s weights frozen.\\nFor some architectures, such as convolutional neural networks, it is common to keep the earlier layers (those closest to the input layer) frozen, as they capture lower-level features, while later layers often discern high-level features that can be more related to the task that the model is trained on.\\nModels that are pre-trained on large, general corpora are usually fine-tuned by reusing their parameters as a starting point and adding a task-specific layer trained from scratch. Fine-tuning the full model is also common and often yields better results, but is more computationally expensive.\\nFine-tuning is typically accomplished via supervised learning, but there are also techniques to fine-tune a model using weak supervision. Fine-tuning can be combined with a reinforcement learning from human feedback-based objective to produce language models such as ChatGPT (a fine-tuned version of GPT models) and Sparrow.\\n\\nPage: List of datasets for machine-learning research\\nSummary: These datasets are used in machine learning (ML) research and have been cited in peer-reviewed academic journals. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets. High-quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce.\\nMany organizations, including governments, publish and share their datasets. The datasets are classified, based on the licenses, as Open data and Non-Open data.\\nThe datasets from various governmental-bodies are presented in List of open government data sites. The datasets are ported on open data portals. They are made available for searching, depositing and accessing through interfaces like Open API.  The datasets are made available as various sorted types and subtypes.', name='wikipedia', id='1949d299-486e-4411-836a-976f10687597', tool_call_id='call_FCvmv7FpjgSqjH1FvYo6PJhT'), ToolMessage(content='Page: Data parallelism\\nSummary: Data parallelism is parallelization across multiple processors in parallel computing environments. It focuses on distributing the data across different nodes, which operate on the data in parallel. It can be applied on regular data structures like arrays and matrices by working on each element in parallel. It contrasts to task parallelism as another form of parallelism.\\nA data parallel job on an array of n elements can be divided equally among all the processors. Let us assume we want to sum all the elements of the given array and the time for a single addition operation is Ta time units. In the case of sequential execution, the time taken by the process will be n×Ta time units as it sums up all the elements of an array. On the other hand, if we execute this job as a data parallel job on 4 processors the time taken would reduce to (n/4)×Ta + merging overhead time units. Parallel execution results in a speedup of 4 over sequential execution. The locality of data references plays an important part in evaluating the performance of a data parallel programming model. Locality of data depends on the memory accesses performed by the program as well as the size of the cache.\\n\\nPage: Ari Holtzman\\nSummary: Ari Holtzman is a professor of Computer Science at the University of Chicago and an expert in the area of Natural language processing and Computational linguistics. Previously, Holtzman was a PhD student at the University of Washington where he was advised by Luke Zettlemoyer.\\nIn 2017, he was a member of the winning team for the inaugural Alexa Prize for developing a conversational AI system for the Amazon Alexa device. Holtzman has made multiple contributions in the area of text generation and language models such as the introduction of nucleus sampling in 2019, his work on AI safety and neural fake news detection, and the fine-tuning of quantized large language models.\\n\\nPage: Bob Dettmer\\nSummary: Robert P. \"Bob\" Dettmer (born March 1, 1951) is a Minnesota politician and former member of the Minnesota House of Representatives. A member of the Republican Party of Minnesota, he represented District 39A, located in the northeastern part of the Twin Cities metro area, which includes Chisago and Washington counties.\\n\\n', name='wikipedia', id='8552e28e-70b6-4a9b-95a7-988a67269965', tool_call_id='call_gw2qwgPWEzS58oUk5AiRkY45'), ToolMessage(content='Page: Attention (machine learning)\\nSummary: Attention is a machine learning method that determines the relative importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by \"soft\" weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size.\\nUnlike \"hard\" weights, which are computed during the backwards training pass, \"soft\" weights exist only in the forward pass and therefore change with every step of the input. Earlier designs implemented the attention mechanism in a serial recurrent neural network (RNN) language translation system, but a more recent design, namely the transformer, removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme.\\nInspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of leveraging information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state.\\n\\nPage: Transformer (deep learning architecture)\\nSummary: The transformer is a deep learning architecture that was developed by researchers at Google and is based on the multi-head attention mechanism, which was proposed in the 2017 paper \"Attention Is All You Need\". Text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.\\nTransformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLM) on large (language) datasets.\\n\\nTransformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multimodal learning, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).\\n\\nPage: Machine learning\\nSummary: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\\nML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\\nStatistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. \\nFrom a theoretical viewpoint, probably approximately correct learning provides a framework for describing machine learning.', name='wikipedia', id='5a553776-0217-47ce-a15e-40cd00713dbf', tool_call_id='call_Ql3pK64VSbXYSEj1nDHmEvfI')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='### LoRA in Machine Learning\\nLoRA (Low-Rank Adaptation) is a technique used in the fine-tuning of deep learning models. It involves augmenting a pre-trained neural network with \"adapters\" that consist of far fewer parameters than the original model. This allows for parameter-efficient fine-tuning by adjusting the weights of these adapters while keeping the rest of the model\\'s weights frozen. This approach is particularly useful for adapting large models to new tasks without the need for extensive computational resources.\\n\\n### Tim Dettmers\\nThere was no specific information found about Tim Dettmers in the search results. It\\'s possible that he is not a widely recognized figure in the available databases or that the search did not yield relevant results. If you have more context or details about his work or field, it might help refine the search.\\n\\n### Attention in Machine Learning\\nAttention is a machine learning method that assigns relative importance to each component in a sequence, such as words in a sentence. In natural language processing, this is represented by \"soft\" weights that change with each input step. The attention mechanism allows a token to access any part of a sentence directly, rather than relying on sequential information as in recurrent neural networks (RNNs). This mechanism is a key component of the transformer architecture, which has become foundational in modern NLP tasks due to its efficiency and effectiveness in handling long-range dependencies in data.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 283, 'prompt_tokens': 2067, 'total_tokens': 2350, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a6889ffe71', 'finish_reason': 'stop', 'logprobs': None}, id='run-cda5f305-da8a-4979-a373-60346c616b0f-0', usage_metadata={'input_tokens': 2067, 'output_tokens': 283, 'total_tokens': 2350, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "d847426e-71b3-47e6-b1ae-351a78d68d1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Prompt Engineering** is the process of crafting instructions to produce the best possible output from a generative AI model. It involves structuring natural language text to describe the task an AI should perform, which can include queries, commands, or longer statements with context and instructions. This technique is used across various AI models, including text-to-text, text-to-image, and text-to-audio models, to guide the AI in generating desired outputs.\n",
            "\n",
            "**History and Emergence:**\n",
            "Prompt engineering has been around since the early days of natural language processing (NLP). It gained significant attention with the release of OpenAI's GPT-3 in 2020, which showcased the potential of large-scale pretrained models. This marked a watershed moment for prompt engineering, as researchers and developers began to explore crafting effective prompts to control and guide the model's behavior. The evolution of large language models (LLMs) and the introduction of reinforcement learning with human feedback (RLHF) further advanced the field, making prompt engineering a critical component in deploying AI systems for various applications.\n",
            "\n",
            "\n",
            "\n",
            "Retrieval-augmented generation (RAG) is a technique that enhances generative artificial intelligence (Gen AI) models by allowing them to retrieve and incorporate new information. This approach modifies interactions with a large language model (LLM) so that the model can respond to user queries by referencing a specified set of documents, supplementing its pre-existing training data with this information. This enables LLMs to use domain-specific and/or updated information, improving their performance and reducing AI hallucinations.\n",
            "\n",
            "RAG was first introduced in 2020 by a team of researchers, including Douwe Kiela, Patrick Lewis, Ethan Perez, and others, in their research paper \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" at Meta.\n",
            "\n",
            "\n",
            "\n",
            "Fine-tuning in deep learning is a transfer learning approach where the parameters of a pre-trained neural network model are trained on new data. This can involve training the entire network or just a subset of its layers, with the rest being \"frozen\" (i.e., not updated during backpropagation). Fine-tuning is often used to adapt models pre-trained on large, general datasets to specific tasks by adding a task-specific layer trained from scratch. This process can be computationally expensive but often yields better results. Fine-tuning can also be combined with reinforcement learning from human feedback to produce models like ChatGPT.\n",
            "\n",
            "The concept of fine-tuning became more prominent with the rise of deep learning and the development of architectures like transformers, which were introduced in the 2017 paper \"Attention Is All You Need\" by researchers at Google. Transformers have since been widely adopted for training large language models and have facilitated the development of pre-trained systems like GPTs and BERT, which are often fine-tuned for specific applications.\n",
            "\n",
            "\n",
            "\n",
            "LLM-based agents refer to intelligent agents that utilize large language models (LLMs) for natural language processing tasks. These agents are designed to perceive their environment, take actions autonomously to achieve goals, and improve their performance through machine learning. LLMs, such as generative pretrained transformers (GPTs), are trained on vast amounts of text and can be fine-tuned for specific tasks or guided by prompt engineering.\n",
            "\n",
            "The concept of LLM-based agents emerged with advancements in large language models, which have enabled more sophisticated interactions and coordination among agents. This development has led to a new area of research in multi-agent systems, where multiple interacting intelligent agents can solve complex problems that are difficult for individual agents or monolithic systems to address.\n",
            "\n",
            "The exact timeline of when LLM-based agents \"broke onto the scene\" is not specified, but it is closely tied to the advancements in LLMs and their applications in AI research and development.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print(messages[\"messages\"][-1].content)\n",
        "  print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
